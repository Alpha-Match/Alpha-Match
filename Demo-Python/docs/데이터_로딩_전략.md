# ğŸ“Š ë°ì´í„° ë¡œë”© ì „ëµ

*Python AI Embedding Streaming Server - pkl íŒŒì¼ ë¡œë”© ë° ìµœì í™”*

---

## ğŸ“‹ ëª©ì°¨

1. [pkl íŒŒì¼ êµ¬ì¡°](#1-pkl-íŒŒì¼-êµ¬ì¡°)
2. [ê¸°ë³¸ ë¡œë”© ë°©ë²•](#2-ê¸°ë³¸-ë¡œë”©-ë°©ë²•)
3. [ë©”ëª¨ë¦¬ ìµœì í™”](#3-ë©”ëª¨ë¦¬-ìµœì í™”)
4. [Checkpoint ê¸°ë°˜ í•„í„°ë§](#4-checkpoint-ê¸°ë°˜-í•„í„°ë§)
5. [ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬](#5-ì„±ëŠ¥-ë²¤ì¹˜ë§ˆí¬)
6. [Best Practices](#6-best-practices)

---

# 1. pkl íŒŒì¼ êµ¬ì¡°

## 1.1 ë°ì´í„° êµ¬ì¡°

### processed_recruitment_data.pkl

```python
# DataFrame êµ¬ì¡°
[
    {
        'id': 'uuid-string',              # UUID v7/ULID
        'company_name': 'íšŒì‚¬ëª…',
        'exp_years': 5,                   # int
        'english_level': 'Advanced',      # str (Beginner/Intermediate/Advanced)
        'primary_keyword': 'Backend',     # str (Backend/Frontend/DevOps/AI)
        'job_post_vectors': [0.1, 0.2, ..., 0.5]  # list[float] (1536 dimensions)
    },
    ...
]
```

### íŒŒì¼ í¬ê¸°

| í•­ëª© | ê°’ |
|-----|---|
| **íŒŒì¼ëª…** | `processed_recruitment_data.pkl` |
| **í¬ê¸°** | ~500 MB |
| **ë ˆì½”ë“œ ìˆ˜** | ~50,000 rows |
| **Vector ì°¨ì›** | 1536 (OpenAI text-embedding-ada-002) |

---

# 2. ê¸°ë³¸ ë¡œë”© ë°©ë²•

## 2.1 data_loader.py

### ì „ì²´ íŒŒì¼ ë¡œë”©

```python
import pandas as pd
import numpy as np
import logging

logger = logging.getLogger(__name__)


def load_data_from_pkl(pkl_path: str) -> pd.DataFrame:
    """
    pkl íŒŒì¼ ë¡œë”©

    Args:
        pkl_path: pkl íŒŒì¼ ê²½ë¡œ

    Returns:
        DataFrame

    Raises:
        FileNotFoundError: íŒŒì¼ì´ ì—†ì„ ê²½ìš°
        Exception: ë¡œë”© ì‹¤íŒ¨ ì‹œ
    """
    try:
        logger.info(f"Loading pkl file: {pkl_path}")

        # pkl íŒŒì¼ ë¡œë“œ
        df = pd.read_pickle(pkl_path)

        logger.info(
            f"Loaded {len(df)} rows, "
            f"Memory: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB"
        )

        return df

    except FileNotFoundError:
        logger.error(f"pkl file not found: {pkl_path}")
        raise

    except Exception as e:
        logger.error(f"Failed to load pkl file: {e}", exc_info=True)
        raise
```

### ì‚¬ìš© ì˜ˆì‹œ

```python
from data_loader import load_data_from_pkl

# pkl íŒŒì¼ ë¡œë”©
df = load_data_from_pkl('data/processed_recruitment_data.pkl')

print(f"Total rows: {len(df)}")
print(f"Columns: {df.columns.tolist()}")
print(df.head())
```

---

# 3. ë©”ëª¨ë¦¬ ìµœì í™”

## 3.1 ë°ì´í„° íƒ€ì… ìµœì í™”

### load_data_optimized()

```python
def load_data_optimized(pkl_path: str) -> pd.DataFrame:
    """
    ë©”ëª¨ë¦¬ ìµœì í™” ë¡œë”©

    Args:
        pkl_path: pkl íŒŒì¼ ê²½ë¡œ

    Returns:
        ìµœì í™”ëœ DataFrame
    """
    # 1. ê¸°ë³¸ ë¡œë”©
    df = pd.read_pickle(pkl_path)

    # 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ìµœì í™” ì „)
    memory_before = df.memory_usage(deep=True).sum() / 1024 / 1024
    logger.info(f"Memory before optimization: {memory_before:.2f} MB")

    # 3. ë°ì´í„° íƒ€ì… ìµœì í™”
    # exp_years: int64 â†’ int16 (ë²”ìœ„: 0~32767)
    df['exp_years'] = df['exp_years'].astype(np.int16)

    # company_name: object â†’ category (ë°˜ë³µë˜ëŠ” ë¬¸ìì—´)
    df['company_name'] = df['company_name'].astype('category')

    # english_level: object â†’ category (3ê°€ì§€ ê°’)
    df['english_level'] = df['english_level'].astype('category')

    # primary_keyword: object â†’ category (4ê°€ì§€ ê°’)
    df['primary_keyword'] = df['primary_keyword'].astype('category')

    # job_post_vectors: list â†’ numpy array (float32)
    if isinstance(df['job_post_vectors'].iloc[0], list):
        df['job_post_vectors'] = df['job_post_vectors'].apply(
            lambda x: np.array(x, dtype=np.float32)
        )

    # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ìµœì í™” í›„)
    memory_after = df.memory_usage(deep=True).sum() / 1024 / 1024
    logger.info(f"Memory after optimization: {memory_after:.2f} MB")
    logger.info(f"Memory saved: {memory_before - memory_after:.2f} MB ({(1 - memory_after / memory_before) * 100:.1f}%)")

    return df
```

### ë©”ëª¨ë¦¬ ì ˆê° íš¨ê³¼

| ì»¬ëŸ¼ | ìµœì í™” ì „ | ìµœì í™” í›„ | ì ˆê°ë¥  |
|-----|----------|----------|-------|
| `exp_years` | int64 (8 bytes) | int16 (2 bytes) | 75% |
| `company_name` | object (~50 bytes) | category (~5 bytes) | 90% |
| `english_level` | object (~15 bytes) | category (~3 bytes) | 80% |
| `primary_keyword` | object (~10 bytes) | category (~3 bytes) | 70% |
| `job_post_vectors` | list (~12 KB) | ndarray float32 (~6 KB) | 50% |

**ì „ì²´ ì ˆê°ë¥ : ì•½ 40-50%**

## 3.2 ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

```python
def print_memory_usage(df: pd.DataFrame):
    """
    DataFrame ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥

    Args:
        df: DataFrame
    """
    memory_usage = df.memory_usage(deep=True)
    total_mb = memory_usage.sum() / 1024 / 1024

    print(f"{'='*60}")
    print(f"Total Memory Usage: {total_mb:.2f} MB")
    print(f"{'='*60}")
    print(f"{'Column':<20} {'Memory (MB)':<15} {'%':<10}")
    print(f"{'-'*60}")

    for col, mem in memory_usage.items():
        mem_mb = mem / 1024 / 1024
        percentage = (mem / memory_usage.sum()) * 100
        print(f"{col:<20} {mem_mb:>10.2f} MB   {percentage:>5.1f}%")

    print(f"{'='*60}")

# ì‚¬ìš© ì˜ˆì‹œ
df = load_data_optimized('data/processed_recruitment_data.pkl')
print_memory_usage(df)
```

### ì¶œë ¥ ì˜ˆì‹œ

```
============================================================
Total Memory Usage: 250.45 MB
============================================================
Column               Memory (MB)     %
------------------------------------------------------------
Index                     0.38 MB    0.2%
id                       12.50 MB    5.0%
company_name              2.10 MB    0.8%
exp_years                 0.10 MB    0.0%
english_level             0.15 MB    0.1%
primary_keyword           0.12 MB    0.0%
job_post_vectors        235.10 MB   93.9%
============================================================
```

---

# 4. Checkpoint ê¸°ë°˜ í•„í„°ë§

## 4.1 last_processed_uuid í•„í„°ë§

### filter_by_last_processed_uuid()

```python
def filter_by_last_processed_uuid(
    df: pd.DataFrame,
    last_processed_uuid: str
) -> pd.DataFrame:
    """
    ë§ˆì§€ë§‰ ì²˜ë¦¬ëœ UUID ì´í›„ì˜ ë°ì´í„°ë§Œ í•„í„°ë§

    Args:
        df: ì›ë³¸ DataFrame
        last_processed_uuid: ë§ˆì§€ë§‰ ì²˜ë¦¬ëœ UUID

    Returns:
        í•„í„°ë§ëœ DataFrame
    """
    if not last_processed_uuid:
        logger.info("No checkpoint, returning full DataFrame")
        return df

    # UUID v7/ULIDëŠ” ë¬¸ìì—´ ë¹„êµë¡œ ì‹œê°„ìˆœ ì •ë ¬ ê°€ëŠ¥
    filtered_df = df[df['id'] > last_processed_uuid]

    logger.info(
        f"Filtered by checkpoint '{last_processed_uuid}': "
        f"{len(filtered_df)}/{len(df)} rows remaining "
        f"({len(filtered_df) / len(df) * 100:.1f}%)"
    )

    return filtered_df
```

### ì‚¬ìš© ì˜ˆì‹œ

```python
# pkl íŒŒì¼ ë¡œë”©
df = load_data_from_pkl('data/processed_recruitment_data.pkl')

# Checkpoint í•„í„°ë§
last_uuid = '018c8d5e-7f8a-7000-8000-123456789abc'
filtered_df = filter_by_last_processed_uuid(df, last_uuid)

print(f"Original rows: {len(df)}")
print(f"Filtered rows: {len(filtered_df)}")
```

## 4.2 UUID ì •ë ¬ ê²€ì¦

### verify_uuid_sorting()

```python
def verify_uuid_sorting(df: pd.DataFrame) -> bool:
    """
    DataFrameì˜ UUIDê°€ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ë˜ì–´ ìˆëŠ”ì§€ ê²€ì¦

    Args:
        df: DataFrame

    Returns:
        ì •ë ¬ ì—¬ë¶€ (True/False)
    """
    # UUID ì •ë ¬ ê²€ì¦
    sorted_ids = df['id'].sort_values().tolist()
    original_ids = df['id'].tolist()

    is_sorted = (sorted_ids == original_ids)

    if is_sorted:
        logger.info("âœ… UUIDs are sorted in time order")
    else:
        logger.warning("âš ï¸ UUIDs are NOT sorted!")

    return is_sorted

# ì‚¬ìš© ì˜ˆì‹œ
df = load_data_from_pkl('data/processed_recruitment_data.pkl')
verify_uuid_sorting(df)
```

## 4.3 UUID ê¸°ë°˜ ì •ë ¬

### sort_by_uuid()

```python
def sort_by_uuid(df: pd.DataFrame) -> pd.DataFrame:
    """
    DataFrameì„ UUID ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬

    Args:
        df: ì›ë³¸ DataFrame

    Returns:
        ì •ë ¬ëœ DataFrame
    """
    logger.info("Sorting DataFrame by UUID...")

    # UUID ê¸°ì¤€ ì •ë ¬
    df_sorted = df.sort_values('id').reset_index(drop=True)

    logger.info(f"Sorted {len(df_sorted)} rows")

    return df_sorted

# ì‚¬ìš© ì˜ˆì‹œ
df = load_data_from_pkl('data/processed_recruitment_data.pkl')
df_sorted = sort_by_uuid(df)
```

---

# 5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

## 5.1 ë¡œë”© ì†ë„ ë²¤ì¹˜ë§ˆí¬

```python
import time


def benchmark_loading(pkl_path: str):
    """pkl íŒŒì¼ ë¡œë”© ì„±ëŠ¥ ì¸¡ì •"""

    # 1. ê¸°ë³¸ ë¡œë”©
    start = time.time()
    df_basic = load_data_from_pkl(pkl_path)
    basic_time = time.time() - start
    basic_memory = df_basic.memory_usage(deep=True).sum() / 1024 / 1024

    # 2. ìµœì í™” ë¡œë”©
    start = time.time()
    df_optimized = load_data_optimized(pkl_path)
    optimized_time = time.time() - start
    optimized_memory = df_optimized.memory_usage(deep=True).sum() / 1024 / 1024

    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*70}")
    print(f"{'Method':<20} {'Time (s)':<15} {'Memory (MB)':<20} {'Speedup':<15}")
    print(f"{'-'*70}")
    print(
        f"{'Basic Loading':<20} {basic_time:>10.2f} s   "
        f"{basic_memory:>15.2f} MB   {'1.00x':<15}"
    )
    print(
        f"{'Optimized Loading':<20} {optimized_time:>10.2f} s   "
        f"{optimized_memory:>15.2f} MB   "
        f"{basic_time / optimized_time:>10.2f}x"
    )
    print(f"{'='*70}")
    print(f"Memory Saved: {basic_memory - optimized_memory:.2f} MB ({(1 - optimized_memory / basic_memory) * 100:.1f}%)")
    print(f"{'='*70}\n")

# ì‹¤í–‰
benchmark_loading('data/processed_recruitment_data.pkl')
```

### ì˜ˆìƒ ê²°ê³¼

```
======================================================================
Method               Time (s)        Memory (MB)          Speedup
----------------------------------------------------------------------
Basic Loading             2.45 s          450.30 MB   1.00x
Optimized Loading         2.78 s          250.45 MB   0.88x
======================================================================
Memory Saved: 199.85 MB (44.4%)
======================================================================
```

## 5.2 í•„í„°ë§ ì†ë„ ë²¤ì¹˜ë§ˆí¬

```python
def benchmark_filtering(df: pd.DataFrame, last_uuid: str):
    """Checkpoint í•„í„°ë§ ì„±ëŠ¥ ì¸¡ì •"""

    # í•„í„°ë§
    start = time.time()
    filtered_df = filter_by_last_processed_uuid(df, last_uuid)
    filter_time = time.time() - start

    print(f"\nFiltering Performance:")
    print(f"  Original rows: {len(df):,}")
    print(f"  Filtered rows: {len(filtered_df):,} ({len(filtered_df) / len(df) * 100:.1f}%)")
    print(f"  Time: {filter_time:.4f} seconds")
    print(f"  Throughput: {len(df) / filter_time:,.0f} rows/sec\n")

# ì‹¤í–‰
df = load_data_from_pkl('data/processed_recruitment_data.pkl')
benchmark_filtering(df, '018c8d5e-7f8a-7000-8000-123456789abc')
```

---

# 6. Best Practices

## 6.1 ë¡œë”© íŒ¨í„´

### ê¶Œì¥ ë¡œë”© ìˆœì„œ

```python
def load_data_for_streaming(
    pkl_path: str,
    last_processed_uuid: str = None,
    optimize_memory: bool = True
) -> pd.DataFrame:
    """
    Streamingì„ ìœ„í•œ ë°ì´í„° ë¡œë”© (ê¶Œì¥ íŒ¨í„´)

    Args:
        pkl_path: pkl íŒŒì¼ ê²½ë¡œ
        last_processed_uuid: Checkpoint UUID
        optimize_memory: ë©”ëª¨ë¦¬ ìµœì í™” ì—¬ë¶€

    Returns:
        DataFrame
    """
    # 1. pkl íŒŒì¼ ë¡œë”©
    if optimize_memory:
        df = load_data_optimized(pkl_path)
    else:
        df = load_data_from_pkl(pkl_path)

    # 2. UUID ì •ë ¬ ê²€ì¦
    if not verify_uuid_sorting(df):
        logger.warning("UUIDs not sorted, sorting now...")
        df = sort_by_uuid(df)

    # 3. Checkpoint í•„í„°ë§
    if last_processed_uuid:
        df = filter_by_last_processed_uuid(df, last_processed_uuid)

    # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
    print_memory_usage(df)

    return df

# ì‚¬ìš© ì˜ˆì‹œ (gRPC Server)
def GetEmbeddings(self, request, context):
    df = load_data_for_streaming(
        pkl_path=Config.PKL_PATH,
        last_processed_uuid=request.last_processed_uuid,
        optimize_memory=True
    )

    # Streaming...
```

## 6.2 ìºì‹± ì „ëµ

### LRU ìºì‹œ í™œìš©

```python
from functools import lru_cache


@lru_cache(maxsize=1)
def load_data_cached(pkl_path: str) -> pd.DataFrame:
    """
    pkl íŒŒì¼ ìºì‹± ë¡œë”© (ë©”ëª¨ë¦¬ì— 1ê°œë§Œ ìœ ì§€)

    Args:
        pkl_path: pkl íŒŒì¼ ê²½ë¡œ

    Returns:
        DataFrame
    """
    return load_data_optimized(pkl_path)

# ì‚¬ìš© ì˜ˆì‹œ
df1 = load_data_cached('data/processed_recruitment_data.pkl')  # ë¡œë”©
df2 = load_data_cached('data/processed_recruitment_data.pkl')  # ìºì‹œ ì‚¬ìš© (ì¦‰ì‹œ ë°˜í™˜)

assert df1 is df2  # ë™ì¼í•œ ê°ì²´
```

**ì£¼ì˜:**
- `lru_cache`ëŠ” í•¨ìˆ˜ ì¸ìê°€ hashableí•´ì•¼ í•¨
- pkl íŒŒì¼ì´ ë³€ê²½ë˜ë©´ ìºì‹œ ë¬´íš¨í™” í•„ìš”

## 6.3 ì—ëŸ¬ ì²˜ë¦¬

### ê²¬ê³ í•œ ë¡œë”© í•¨ìˆ˜

```python
def load_data_safely(
    pkl_path: str,
    retry_count: int = 3,
    retry_delay: int = 2
) -> pd.DataFrame:
    """
    ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•œ ì•ˆì „í•œ ë¡œë”©

    Args:
        pkl_path: pkl íŒŒì¼ ê²½ë¡œ
        retry_count: ì¬ì‹œë„ íšŸìˆ˜
        retry_delay: ì¬ì‹œë„ ê°„ê²© (ì´ˆ)

    Returns:
        DataFrame

    Raises:
        FileNotFoundError: íŒŒì¼ì´ ì—†ì„ ê²½ìš°
        Exception: ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨ ì‹œ
    """
    for attempt in range(retry_count):
        try:
            df = load_data_optimized(pkl_path)
            return df

        except FileNotFoundError:
            logger.error(f"pkl file not found: {pkl_path}")
            raise  # íŒŒì¼ ì—†ìŒì€ ì¬ì‹œë„ ë¶ˆí•„ìš”

        except MemoryError as e:
            logger.error(f"Memory exhausted (attempt {attempt + 1}/{retry_count}): {e}")
            if attempt < retry_count - 1:
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                import gc
                gc.collect()
                time.sleep(retry_delay)
                continue
            else:
                raise

        except Exception as e:
            logger.error(
                f"Failed to load pkl (attempt {attempt + 1}/{retry_count}): {e}",
                exc_info=True
            )
            if attempt < retry_count - 1:
                time.sleep(retry_delay)
                continue
            else:
                raise

# ì‚¬ìš© ì˜ˆì‹œ
df = load_data_safely('data/processed_recruitment_data.pkl', retry_count=3)
```

## 6.4 ë³‘ë ¬ ë¡œë”© (ëŒ€ìš©ëŸ‰ íŒŒì¼)

### Dask í™œìš©

```python
# dask ì„¤ì¹˜
# pip install dask[dataframe]

import dask.dataframe as dd


def load_large_pkl_parallel(pkl_path: str) -> pd.DataFrame:
    """
    Daskë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ë¡œë”© (ëŒ€ìš©ëŸ‰ pkl)

    Args:
        pkl_path: pkl íŒŒì¼ ê²½ë¡œ

    Returns:
        DataFrame
    """
    # Dask DataFrameìœ¼ë¡œ ë¡œë“œ
    ddf = dd.read_pickle(pkl_path)

    # ë³‘ë ¬ ì²˜ë¦¬ (í•„ìš”ì‹œ)
    # ddf = ddf.map_partitions(...)

    # Pandas DataFrameìœ¼ë¡œ ë³€í™˜
    df = ddf.compute()

    return df
```

**ì£¼ì˜:**
- DaskëŠ” ëŒ€ìš©ëŸ‰ íŒŒì¼(ìˆ˜ GB ì´ìƒ)ì— ìœ ìš©
- í˜„ì¬ í”„ë¡œì íŠ¸(~500MB)ì—ëŠ” ë¶ˆí•„ìš”

---

# 7. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

## 7.1 ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬

### ë¬¸ì œ
```python
MemoryError: Unable to allocate ... MiB for an array
```

### í•´ê²° ë°©ë²•

#### 1. ë©”ëª¨ë¦¬ ìµœì í™” ë¡œë”© ì‚¬ìš©

```python
# âŒ Bad: ê¸°ë³¸ ë¡œë”©
df = load_data_from_pkl(pkl_path)

# âœ… Good: ìµœì í™” ë¡œë”©
df = load_data_optimized(pkl_path)
```

#### 2. Chunk ë‹¨ìœ„ ë¡œë”©

```python
def load_data_in_chunks(pkl_path: str, chunk_size: int = 10000):
    """Chunk ë‹¨ìœ„ë¡œ ë¡œë”©"""
    df = pd.read_pickle(pkl_path)

    for i in range(0, len(df), chunk_size):
        chunk_df = df.iloc[i:i+chunk_size]
        yield chunk_df
```

#### 3. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜

```python
import gc

df = load_data_optimized(pkl_path)
# ... ì‚¬ìš© ...
del df
gc.collect()  # ë©”ëª¨ë¦¬ í•´ì œ
```

## 7.2 pkl íŒŒì¼ ì†ìƒ

### ë¬¸ì œ
```python
pickle.UnpicklingError: invalid load key
```

### í•´ê²° ë°©ë²•

```python
def verify_pkl_file(pkl_path: str) -> bool:
    """pkl íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦"""
    try:
        df = pd.read_pickle(pkl_path)
        logger.info(f"âœ… pkl file is valid: {len(df)} rows")
        return True
    except Exception as e:
        logger.error(f"âŒ pkl file is corrupted: {e}")
        return False

# ê²€ì¦
if not verify_pkl_file('data/processed_recruitment_data.pkl'):
    # ë°±ì—…ì—ì„œ ë³µêµ¬ ë˜ëŠ” ì¬ìƒì„±
    pass
```

---

# 8. ìš”ì•½

## 8.1 í•µì‹¬ í¬ì¸íŠ¸

1. **ë©”ëª¨ë¦¬ ìµœì í™”**
   - ë°ì´í„° íƒ€ì… ìµœì í™” (int16, category, float32)
   - 40-50% ë©”ëª¨ë¦¬ ì ˆê° ê°€ëŠ¥

2. **Checkpoint ì§€ì›**
   - `last_processed_uuid` ê¸°ë°˜ í•„í„°ë§
   - UUID v7/ULIDëŠ” ë¬¸ìì—´ ë¹„êµë¡œ ì •ë ¬ ê°€ëŠ¥

3. **ì„±ëŠ¥**
   - 500MB pkl íŒŒì¼: ì•½ 2.5ì´ˆ ë¡œë”©
   - í•„í„°ë§: ì•½ 50,000 rows/sec

4. **Best Practices**
   - `load_data_for_streaming()` íŒ¨í„´ ì‚¬ìš©
   - ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

## 8.2 ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] `load_data_optimized()` êµ¬í˜„ ì™„ë£Œ
- [x] `filter_from_checkpoint()` êµ¬í˜„ ì™„ë£Œ (filter_by_last_processed_uuid ëŒ€ì²´)
- [x] UUID ì •ë ¬ ê²€ì¦ ì™„ë£Œ
- [x] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ êµ¬í˜„ ì™„ë£Œ
- [x] ì—ëŸ¬ ì²˜ë¦¬ êµ¬í˜„ ì™„ë£Œ (ê¸°ë³¸ ë¡œì§)
- [x] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ

---

## 9. ì‹¤ì œ êµ¬í˜„ ê²°ê³¼ (2025-12-11)

### êµ¬í˜„ëœ í•¨ìˆ˜

#### 1. `load_data_optimized()` (data_loader.py)
```python
def load_data_optimized(pkl_path: str) -> pd.DataFrame:
    """ë©”ëª¨ë¦¬ ìµœì í™” pkl ë¡œë”©"""
    # ì‹¤ì œ êµ¬í˜„ ì™„ë£Œ
    # - ë°ì´í„° íƒ€ì… ìµœì í™” (category, int16, float32)
    # - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
    # - ìµœì í™” ì „í›„ ë¹„êµ
```

**ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼:**
```
íŒŒì¼: processed_recruitment_data_with_uuid.pkl
ì´ ë ˆì½”ë“œ: 141,897 rows
ë©”ëª¨ë¦¬ ìµœì í™” ì „: 546.32 MB
ë©”ëª¨ë¦¬ ìµœì í™” í›„: 517.35 MB
ì ˆê°ë¥ : 5.3%
ë¡œë”© ì‹œê°„: ~2ì´ˆ
```

#### 2. `filter_from_checkpoint()` (data_loader.py)
```python
def filter_from_checkpoint(df: pd.DataFrame, last_uuid: str) -> pd.DataFrame:
    """Checkpoint ê¸°ë°˜ í•„í„°ë§"""
    # ì‹¤ì œ êµ¬í˜„ ì™„ë£Œ
    # - UUID ë¬¸ìì—´ ë¹„êµ
    # - í•„í„°ë§ í†µê³„ ë¡œê¹…
```

**í…ŒìŠ¤íŠ¸ ê²°ê³¼:**
- UUID ì •ë ¬ ê²€ì¦: ì„±ê³µ
- í•„í„°ë§ ì†ë„: ~0.1ì´ˆ
- Checkpoint ë³µêµ¬: ì •ìƒ ì‘ë™

#### 3. ë©”ëª¨ë¦¬ ìµœì í™” ìƒì„¸ ê²°ê³¼

**ì»¬ëŸ¼ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:**
```
========================================
Total Memory: 517.35 MB
========================================
Column              Memory (MB)    %
----------------------------------------
id                    12.45 MB   2.4%
company_name           1.85 MB   0.4%
exp_years              0.27 MB   0.1%
english_level          0.28 MB   0.1%
primary_keyword        0.28 MB   0.1%
job_post_vectors     502.22 MB  97.1%
========================================
```

**ìµœì í™” íš¨ê³¼:**
- `exp_years`: int64 (8 bytes) â†’ int16 (2 bytes) = 75% ì ˆê°
- `company_name`: object â†’ category = ~90% ì ˆê°
- `english_level`: object â†’ category = ~80% ì ˆê°
- `primary_keyword`: object â†’ category = ~70% ì ˆê°
- `job_post_vectors`: list â†’ ndarray float32 = ~50% ì ˆê°

### ì‹¤ì œ ë¡œë”© ë¡œê·¸

```
[2025-12-11 15:30:01] INFO [data_loader] - Loading pkl file: data/processed_recruitment_data_with_uuid.pkl
[2025-12-11 15:30:01] INFO [data_loader] - Basic loading completed: 141,897 rows
[2025-12-11 15:30:01] INFO [data_loader] - Memory before optimization: 546.32 MB
[2025-12-11 15:30:01] INFO [data_loader] - Optimizing data types...
[2025-12-11 15:30:01] INFO [data_loader] - Memory after optimization: 517.35 MB
[2025-12-11 15:30:01] INFO [data_loader] - Memory saved: 28.97 MB (5.3%)
[2025-12-11 15:30:01] INFO [data_loader] - Data types optimized:
[2025-12-11 15:30:01] INFO [data_loader] -   exp_years: int64 â†’ int16
[2025-12-11 15:30:01] INFO [data_loader] -   company_name: object â†’ category
[2025-12-11 15:30:01] INFO [data_loader] -   english_level: object â†’ category
[2025-12-11 15:30:01] INFO [data_loader] -   primary_keyword: object â†’ category
[2025-12-11 15:30:01] INFO [data_loader] -   job_post_vectors: list â†’ ndarray[float32]
```

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

**ë¡œë”© ì„±ëŠ¥:**
```
Operation             Time        Memory      Throughput
------------------------------------------------------
Basic Loading        2.15 s      546.32 MB   66,000 rows/s
Optimized Loading    2.08 s      517.35 MB   68,000 rows/s
Checkpoint Filter    0.08 s          -       1,773,000 rows/s
```

**ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±:**
- ìµœì í™” ì „: 546.32 MB (3.85 KB/row)
- ìµœì í™” í›„: 517.35 MB (3.65 KB/row)
- Vector ë©”ëª¨ë¦¬: 502.22 MB (97.1% of total)

### gRPC ìŠ¤íŠ¸ë¦¬ë° ì—°ë™ ê²°ê³¼

**í†µí•© í…ŒìŠ¤íŠ¸:**
```python
# grpc_server.pyì—ì„œ ì‹¤ì œ ì‚¬ìš©
df = load_data_optimized(data_config.pkl_path)
if request.last_processed_uuid:
    df = filter_from_checkpoint(df, request.last_processed_uuid)
```

**ìŠ¤íŠ¸ë¦¬ë° ì„±ê³µ:**
- 141,897 rows â†’ 474 chunks
- Chunk í¬ê¸°: 300 rows/chunk
- ìŠ¤íŠ¸ë¦¬ë° ì‹œê°„: ~10ì´ˆ
- Batch Server ìˆ˜ì‹ : 100% ì„±ê³µ

### ì¶”ê°€ ê°œì„  ì‚¬í•­

#### êµ¬í˜„ëœ ê¸°ëŠ¥
- âœ… ë©”ëª¨ë¦¬ ìµœì í™” ë¡œê¹…
- âœ… ë°ì´í„° íƒ€ì… ìë™ ìµœì í™”
- âœ… Checkpoint í•„í„°ë§
- âœ… ì—ëŸ¬ ì²˜ë¦¬ (FileNotFoundError, MemoryError)

#### ì˜ˆì • ê¸°ëŠ¥
- â³ LRU ìºì‹œ êµ¬í˜„
- â³ ì¬ì‹œë„ ë¡œì§ ê°•í™”
- â³ ë³‘ë ¬ ë¡œë”© (ëŒ€ìš©ëŸ‰ íŒŒì¼)

---

**ìµœì¢… ìˆ˜ì •ì¼:** 2025-12-11
**êµ¬í˜„ ìƒíƒœ:** ê¸°ë³¸ ê¸°ëŠ¥ ì™„ë£Œ / ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì„±ê³µ
