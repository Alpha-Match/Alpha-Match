# ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ì „ëµ

*Python AI Embedding Streaming Server - gRPC Streaming ìµœì í™”*

---

## ğŸ“‹ ëª©ì°¨

1. [Streaming ê°œìš”](#1-streaming-ê°œìš”)
2. [Chunk ë¶„í•  ì „ëµ](#2-chunk-ë¶„í• -ì „ëµ)
3. [Backpressure ì²˜ë¦¬](#3-backpressure-ì²˜ë¦¬)
4. [ì„±ëŠ¥ ìµœì í™”](#4-ì„±ëŠ¥-ìµœì í™”)
5. [ì—ëŸ¬ ë³µêµ¬](#5-ì—ëŸ¬-ë³µêµ¬)
6. [ëª¨ë‹ˆí„°ë§](#6-ëª¨ë‹ˆí„°ë§)

---

# 1. Streaming ê°œìš”

## 1.1 ì™œ Streamingì¸ê°€?

### ì¼ë°˜ RPC vs Streaming RPC

#### ì¼ë°˜ RPC (Unary RPC)

```python
# âŒ Bad: ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì „ì†¡
def SendAllEmbeddings(request):
    all_embeddings = load_all_data()  # 500MB ë©”ëª¨ë¦¬ ì‚¬ìš©
    return EmbeddingResponse(embeddings=all_embeddings)
```

**ë¬¸ì œì :**
- ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)
- ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ
- ì¬ì‹œì‘ ì‹œ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì „ì†¡

#### Streaming RPC

```python
# âœ… Good: Chunk ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì „ì†¡
def StreamEmbeddings(request):
    for chunk in load_data_in_chunks():  # 3MBì”© ë©”ëª¨ë¦¬ ì‚¬ìš©
        yield EmbeddingChunk(embeddings=chunk)
```

**ì´ì :**
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (Chunk í¬ê¸°ë§Œí¼ë§Œ ë©”ëª¨ë¦¬ ì‚¬ìš©)
- Backpressure ì§€ì› (ìˆ˜ì‹ ì ì†ë„ì— ë§ì¶° ì „ì†¡)
- Checkpoint ì§€ì› (ì¤‘ê°„ë¶€í„° ì¬ì‹œì‘ ê°€ëŠ¥)

## 1.2 Alpha-Matchì˜ Streaming íŒ¨í„´

### Server Streaming

```
Python Server (Producer)
    â”‚
    â”œâ”€â”€> Chunk 1 (300 rows) â”€â”€â”€â”€â”€â”€â”
    â”œâ”€â”€> Chunk 2 (300 rows) â”€â”€â”€â”€â”€â”€â”¤
    â”œâ”€â”€> Chunk 3 (300 rows) â”€â”€â”€â”€â”€â”€â”¤â”€â”€> Batch Server (Consumer)
    â””â”€â”€> ...                      â”‚
                                  â†“
                            Database (pgvector)
```

### Client Streaming

```
Python Server (Producer)
    â”‚
    â”œâ”€â”€> Chunk 1 (300 rows) â”€â”€â”€â”€â”€â”€â”
    â”œâ”€â”€> Chunk 2 (300 rows) â”€â”€â”€â”€â”€â”€â”¤
    â”œâ”€â”€> Chunk 3 (300 rows) â”€â”€â”€â”€â”€â”€â”¤â”€â”€> Batch Server (Consumer)
    â””â”€â”€> onCompleted()            â”‚
                                  â†“
                            UploadResult ë°˜í™˜
```

---

# 2. Chunk ë¶„í•  ì „ëµ

## 2.1 chunker.py êµ¬í˜„

### ê¸°ë³¸ Chunker

```python
from typing import Iterator
import pandas as pd
import logging

logger = logging.getLogger(__name__)


def chunker(df: pd.DataFrame, chunk_size: int) -> Iterator[pd.DataFrame]:
    """
    DataFrameì„ Chunkë¡œ ë¶„í• 

    Args:
        df: ì›ë³¸ DataFrame
        chunk_size: Chunk í¬ê¸° (row ìˆ˜)

    Yields:
        chunk_df: Chunk DataFrame
    """
    total_rows = len(df)
    num_chunks = (total_rows + chunk_size - 1) // chunk_size  # ì˜¬ë¦¼

    logger.info(
        f"Chunking {total_rows} rows into {num_chunks} chunks "
        f"(chunk_size={chunk_size})"
    )

    for i in range(0, total_rows, chunk_size):
        chunk_df = df.iloc[i:i+chunk_size]
        yield chunk_df

# ì‚¬ìš© ì˜ˆì‹œ
df = pd.read_pickle('data/processed_recruitment_data.pkl')

for chunk_num, chunk_df in enumerate(chunker(df, chunk_size=300), start=1):
    print(f"Chunk {chunk_num}: {len(chunk_df)} rows")
    # gRPC Streaming...
```

## 2.2 ë™ì  Chunk í¬ê¸° ê³„ì‚°

### calculate_optimal_chunk_size()

```python
def calculate_optimal_chunk_size(
    vector_dimension: int = 1536,
    available_memory_mb: float = 100,
    safety_factor: float = 0.5
) -> int:
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì  Chunk í¬ê¸° ê³„ì‚°

    Args:
        vector_dimension: Vector ì°¨ì› (ê¸°ë³¸ 1536)
        available_memory_mb: ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (MB)
        safety_factor: ì•ˆì „ ì—¬ìœ ìœ¨ (0.5 = 50%)

    Returns:
        ìµœì  Chunk í¬ê¸° (row ìˆ˜)
    """
    # 1. Vector ë©”ëª¨ë¦¬ (float32 = 4 bytes)
    vector_bytes = vector_dimension * 4

    # 2. Metadata ëŒ€ëµ 500 bytesë¡œ ê°€ì •
    metadata_bytes = 500

    # 3. ì´ row ë‹¹ ë©”ëª¨ë¦¬
    bytes_per_row = vector_bytes + metadata_bytes

    # 4. ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (ì•ˆì „ ì—¬ìœ ìœ¨ ì ìš©)
    usable_bytes = available_memory_mb * 1024 * 1024 * safety_factor

    # 5. ìµœì  Chunk í¬ê¸°
    optimal_size = int(usable_bytes / bytes_per_row)

    # 6. ìµœì†Œ 10, ìµœëŒ€ 1000ìœ¼ë¡œ ì œí•œ
    optimal_size = max(10, min(optimal_size, 1000))

    logger.info(
        f"Calculated optimal chunk size: {optimal_size} "
        f"(vector_dim={vector_dimension}, "
        f"available_mem={available_memory_mb}MB, "
        f"safety_factor={safety_factor})"
    )

    return optimal_size

# ì‚¬ìš© ì˜ˆì‹œ
chunk_size = calculate_optimal_chunk_size(
    vector_dimension=1536,
    available_memory_mb=100,  # 100MB ì‚¬ìš© ê°€ëŠ¥
    safety_factor=0.5         # 50% ì—¬ìœ 
)
print(f"Optimal chunk size: {chunk_size}")  # ì•½ 300
```

### Chunk í¬ê¸°ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

| Chunk í¬ê¸° | Vector ë©”ëª¨ë¦¬ (1536ì°¨ì›) | Metadata ë©”ëª¨ë¦¬ | ì´ ë©”ëª¨ë¦¬ |
|-----------|------------------------|----------------|----------|
| 10 | ~60 KB | ~5 KB | ~65 KB |
| 100 | ~600 KB | ~50 KB | ~650 KB |
| **300** | **~1.8 MB** | **~150 KB** | **~2 MB** â† ê¶Œì¥ |
| 500 | ~3 MB | ~250 KB | ~3.5 MB |
| 1000 | ~6 MB | ~500 KB | ~7 MB |

## 2.3 Adaptive Chunking

### ë„¤íŠ¸ì›Œí¬ ìƒí™©ì— ë”°ë¥¸ ë™ì  ì¡°ì •

```python
import time


class AdaptiveChunker:
    """ë„¤íŠ¸ì›Œí¬ ìƒí™©ì— ë”°ë¼ Chunk í¬ê¸°ë¥¼ ë™ì  ì¡°ì •"""

    def __init__(
        self,
        initial_chunk_size: int = 300,
        min_chunk_size: int = 50,
        max_chunk_size: int = 1000,
        target_latency_ms: float = 100.0
    ):
        self.chunk_size = initial_chunk_size
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        self.target_latency_ms = target_latency_ms

        self.latency_history = []
        self.max_history_size = 10

    def adjust_chunk_size(self, latency_ms: float):
        """
        ë ˆì´í„´ì‹œ ê¸°ë°˜ Chunk í¬ê¸° ì¡°ì •

        Args:
            latency_ms: ìµœê·¼ Chunk ì „ì†¡ ë ˆì´í„´ì‹œ (ms)
        """
        self.latency_history.append(latency_ms)

        # ìµœê·¼ 10ê°œ í‰ê· 
        if len(self.latency_history) > self.max_history_size:
            self.latency_history.pop(0)

        avg_latency = sum(self.latency_history) / len(self.latency_history)

        # ë ˆì´í„´ì‹œê°€ ëª©í‘œë³´ë‹¤ ë†’ìœ¼ë©´ Chunk í¬ê¸° ê°ì†Œ
        if avg_latency > self.target_latency_ms:
            self.chunk_size = max(
                self.min_chunk_size,
                int(self.chunk_size * 0.9)
            )
            logger.info(f"Decreased chunk size to {self.chunk_size} (latency: {avg_latency:.2f}ms)")

        # ë ˆì´í„´ì‹œê°€ ëª©í‘œë³´ë‹¤ ë‚®ìœ¼ë©´ Chunk í¬ê¸° ì¦ê°€
        elif avg_latency < self.target_latency_ms * 0.5:
            self.chunk_size = min(
                self.max_chunk_size,
                int(self.chunk_size * 1.1)
            )
            logger.info(f"Increased chunk size to {self.chunk_size} (latency: {avg_latency:.2f}ms)")

    def chunker(self, df: pd.DataFrame) -> Iterator[pd.DataFrame]:
        """Adaptive Chunking"""
        total_rows = len(df)
        i = 0

        while i < total_rows:
            start_time = time.time()

            # í˜„ì¬ Chunk í¬ê¸°ë¡œ ë¶„í• 
            chunk_df = df.iloc[i:i+self.chunk_size]
            yield chunk_df

            i += self.chunk_size

            # ë ˆì´í„´ì‹œ ì¸¡ì • ë° ì¡°ì •
            latency_ms = (time.time() - start_time) * 1000
            self.adjust_chunk_size(latency_ms)

# ì‚¬ìš© ì˜ˆì‹œ
df = pd.read_pickle('data/processed_recruitment_data.pkl')
adaptive_chunker = AdaptiveChunker(initial_chunk_size=300)

for chunk_num, chunk_df in enumerate(adaptive_chunker.chunker(df), start=1):
    print(f"Chunk {chunk_num}: {len(chunk_df)} rows (chunk_size={adaptive_chunker.chunk_size})")
```

---

# 3. Backpressure ì²˜ë¦¬

## 3.1 gRPCì˜ Backpressure

### gRPCì˜ ìë™ Backpressure

gRPCëŠ” ë‚´ì¥ëœ **Flow Control** ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ìë™ìœ¼ë¡œ Backpressureë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
def GetEmbeddings(self, request, context):
    """gRPCê°€ ìë™ìœ¼ë¡œ Backpressure ì²˜ë¦¬"""
    for chunk_df in chunker(df, chunk_size):
        for _, row in chunk_df.iterrows():
            embedding = create_embedding(row)

            # yield ì‹œ ìˆ˜ì‹ ìê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìë™ Backpressure)
            yield embedding
```

**ë™ì‘ ë°©ì‹:**
1. ìˆ˜ì‹ ì(Batch Server)ê°€ ì²˜ë¦¬ ì¤‘ì´ë©´ ì†¡ì‹ ì(Python Server) ëŒ€ê¸°
2. ìˆ˜ì‹ ìê°€ ì¤€ë¹„ë˜ë©´ ì†¡ì‹ ì ì¬ê°œ
3. ë„¤íŠ¸ì›Œí¬ ë²„í¼ê°€ ê°€ë“ ì°¨ë©´ ìë™ìœ¼ë¡œ ì†¡ì‹  ì¤‘ë‹¨

## 3.2 ìˆ˜ë™ Backpressure ì œì–´

### ë ˆì´í„´ì‹œ ê¸°ë°˜ ëŒ€ê¸°

```python
import time


def GetEmbeddings(self, request, context):
    """ë ˆì´í„´ì‹œ ê¸°ë°˜ ìˆ˜ë™ Backpressure"""
    last_send_time = time.time()
    min_interval_ms = 10  # ìµœì†Œ 10ms ê°„ê²©

    for chunk_df in chunker(df, chunk_size):
        for _, row in chunk_df.iterrows():
            embedding = create_embedding(row)

            # ìµœì†Œ ê°„ê²© ìœ ì§€ (ê³¼ë„í•œ ì „ì†¡ ë°©ì§€)
            elapsed_ms = (time.time() - last_send_time) * 1000
            if elapsed_ms < min_interval_ms:
                time.sleep((min_interval_ms - elapsed_ms) / 1000)

            yield embedding
            last_send_time = time.time()
```

## 3.3 Buffer í¬ê¸° ì œì–´

### gRPC Options

```python
def serve():
    options = [
        # HTTP/2 ì´ˆê¸° ìœˆë„ìš° í¬ê¸° (64KB â†’ 1MB)
        ('grpc.http2.initial_window_size', 1024 * 1024),

        # Connection ë ˆë²¨ ìœˆë„ìš° í¬ê¸° (64KB â†’ 4MB)
        ('grpc.http2.max_frame_size', 4 * 1024 * 1024),

        # ìµœëŒ€ ë™ì‹œ ìŠ¤íŠ¸ë¦¼ ìˆ˜
        ('grpc.max_concurrent_streams', 100),
    ]

    server = grpc.server(
        futures.ThreadPoolExecutor(max_workers=10),
        options=options
    )
```

---

# 4. ì„±ëŠ¥ ìµœì í™”

## 4.1 ë³‘ë ¬ Chunk ì²˜ë¦¬

### ThreadPoolExecutor í™œìš©

```python
from concurrent.futures import ThreadPoolExecutor


def create_embedding_parallel(row):
    """Embedding ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    embedding = embedding_stream_pb2.Embedding(
        id=str(row['id']),
        company_name=row['company_name'],
        exp_years=int(row['exp_years']),
        english_level=row['english_level'],
        primary_keyword=row['primary_keyword'],
        vector=row['job_post_vectors'].tolist()
    )
    return embedding


def GetEmbeddings(self, request, context):
    """ë³‘ë ¬ Chunk ì²˜ë¦¬"""
    df = load_data_from_pkl(Config.PKL_PATH)

    with ThreadPoolExecutor(max_workers=4) as executor:
        for chunk_df in chunker(df, chunk_size):
            # ë³‘ë ¬ë¡œ Embedding ìƒì„±
            embeddings = list(
                executor.map(create_embedding_parallel, chunk_df.iterrows())
            )

            # ì „ì†¡
            for embedding in embeddings:
                yield embedding
```

**ì£¼ì˜:**
- Python GILë¡œ ì¸í•´ CPU-bound ì‘ì—…ì—ì„œëŠ” íš¨ê³¼ ì œí•œì 
- I/O-bound ì‘ì—… (ë„¤íŠ¸ì›Œí¬)ì—ì„œëŠ” íš¨ê³¼ì 

## 4.2 ë°°ì¹˜ ì „ì†¡

### ì—¬ëŸ¬ Embeddingì„ í•œ ë²ˆì— ì „ì†¡

```protobuf
// proto ì •ì˜ ìˆ˜ì •
message EmbeddingBatch {
  repeated Embedding embeddings = 1;
}

service EmbeddingStreamService {
  rpc GetEmbeddings (RequestParams) returns (stream EmbeddingBatch);
}
```

```python
def GetEmbeddings(self, request, context):
    """ë°°ì¹˜ ì „ì†¡ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ)"""
    df = load_data_from_pkl(Config.PKL_PATH)
    batch_size = 10  # 10ê°œì”© ë°°ì¹˜

    for chunk_df in chunker(df, chunk_size):
        embeddings = []

        for _, row in chunk_df.iterrows():
            embedding = create_embedding(row)
            embeddings.append(embedding)

            # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì „ì†¡
            if len(embeddings) >= batch_size:
                yield embedding_stream_pb2.EmbeddingBatch(embeddings=embeddings)
                embeddings = []

        # ë‚¨ì€ Embedding ì „ì†¡
        if embeddings:
            yield embedding_stream_pb2.EmbeddingBatch(embeddings=embeddings)
```

**ì´ì :**
- ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ (ë©”ì‹œì§€ ìˆ˜ ê°ì†Œ)
- ì²˜ë¦¬ëŸ‰ ì¦ê°€ (ì•½ 20-30%)

## 4.3 ë©”ëª¨ë¦¬ í’€

### ê°ì²´ ì¬ì‚¬ìš©

```python
from queue import Queue


class EmbeddingPool:
    """Embedding ê°ì²´ í’€"""

    def __init__(self, pool_size: int = 100):
        self.pool = Queue(maxsize=pool_size)

        # ì´ˆê¸° ê°ì²´ ìƒì„±
        for _ in range(pool_size):
            self.pool.put(embedding_stream_pb2.Embedding())

    def get(self):
        """ê°ì²´ ê°€ì ¸ì˜¤ê¸°"""
        return self.pool.get()

    def put(self, embedding):
        """ê°ì²´ ë°˜í™˜"""
        # ì¬ì‚¬ìš© ì „ ì´ˆê¸°í™”
        embedding.Clear()
        self.pool.put(embedding)


# ì‚¬ìš© ì˜ˆì‹œ
pool = EmbeddingPool(pool_size=100)

def GetEmbeddings(self, request, context):
    for chunk_df in chunker(df, chunk_size):
        for _, row in chunk_df.iterrows():
            # í’€ì—ì„œ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
            embedding = pool.get()

            # ë°ì´í„° ì„¤ì •
            embedding.id = str(row['id'])
            embedding.company_name = row['company_name']
            # ...

            yield embedding

            # í’€ì— ë°˜í™˜
            pool.put(embedding)
```

**ì´ì :**
- GC ì••ë°• ê°ì†Œ
- ë©”ëª¨ë¦¬ í• ë‹¹/í•´ì œ ì˜¤ë²„í—¤ë“œ ê°ì†Œ

---

# 5. ì—ëŸ¬ ë³µêµ¬

## 5.1 Checkpoint ê¸°ë°˜ ì¬ì‹œì‘

### Server Side

```python
def GetEmbeddings(self, request, context):
    """Checkpoint ì§€ì›"""
    try:
        df = load_data_from_pkl(Config.PKL_PATH)

        # Checkpoint í•„í„°ë§
        if request.last_processed_uuid:
            df = filter_by_last_processed_uuid(df, request.last_processed_uuid)
            logger.info(f"Resuming from UUID: {request.last_processed_uuid}")

        # Streaming...
        for chunk_df in chunker(df, chunk_size):
            for _, row in chunk_df.iterrows():
                embedding = create_embedding(row)
                yield embedding

                # ë§ˆì§€ë§‰ ì „ì†¡ UUID ë¡œê¹… (ì¬ì‹œì‘ ì‹œ ì°¸ê³ )
                logger.debug(f"Sent UUID: {embedding.id}")

    except Exception as e:
        logger.error(f"Streaming error: {e}", exc_info=True)
        context.set_code(grpc.StatusCode.INTERNAL)
        context.set_details(f'Streaming error: {str(e)}')
        return
```

### Client Side (ì¬ì‹œì‘ ë¡œì§)

```python
def upload_with_checkpoint(
    max_retries: int = 3,
    checkpoint_file: str = 'checkpoint.txt'
):
    """Checkpoint ê¸°ë°˜ ì¬ì‹œì‘ ì—…ë¡œë“œ"""
    # ì´ì „ Checkpoint ë¡œë“œ
    last_processed_uuid = load_checkpoint(checkpoint_file)

    for attempt in range(max_retries):
        try:
            logger.info(f"Starting upload (last_uuid={last_processed_uuid})")

            # Request ìƒì„±
            request = embedding_stream_pb2.RequestParams(
                last_processed_uuid=last_processed_uuid,
                chunk_size=300
            )

            # Streaming í˜¸ì¶œ
            responses = stub.GetEmbeddings(request)

            for response in responses:
                # ì²˜ë¦¬...
                last_processed_uuid = response.id

                # Checkpoint ì €ì¥ (ì£¼ê¸°ì ìœ¼ë¡œ)
                if random.random() < 0.01:  # 1% í™•ë¥ 
                    save_checkpoint(checkpoint_file, last_processed_uuid)

            # ì™„ë£Œ í›„ Checkpoint ì‚­ì œ
            delete_checkpoint(checkpoint_file)
            logger.info("Upload completed successfully")
            return

        except grpc.RpcError as e:
            logger.error(f"Upload failed (attempt {attempt + 1}/{max_retries}): {e}")

            if attempt < max_retries - 1:
                # Checkpoint ì €ì¥ í›„ ì¬ì‹œë„
                save_checkpoint(checkpoint_file, last_processed_uuid)
                time.sleep(2)
                continue
            else:
                raise


def load_checkpoint(checkpoint_file: str) -> str:
    """Checkpoint ë¡œë“œ"""
    try:
        with open(checkpoint_file, 'r') as f:
            return f.read().strip()
    except FileNotFoundError:
        return ''


def save_checkpoint(checkpoint_file: str, uuid: str):
    """Checkpoint ì €ì¥"""
    with open(checkpoint_file, 'w') as f:
        f.write(uuid)
    logger.debug(f"Checkpoint saved: {uuid}")


def delete_checkpoint(checkpoint_file: str):
    """Checkpoint ì‚­ì œ"""
    import os
    if os.path.exists(checkpoint_file):
        os.remove(checkpoint_file)
        logger.info("Checkpoint deleted")
```

## 5.2 ìë™ ì¬ì—°ê²°

### Exponential Backoff

```python
import time
import random


def upload_with_exponential_backoff(
    max_retries: int = 5,
    base_delay: int = 1,
    max_delay: int = 60
):
    """Exponential Backoff ì¬ì‹œë„"""
    for attempt in range(max_retries):
        try:
            result = upload_embeddings_to_batch_server()
            return result

        except grpc.RpcError as e:
            if e.code() == grpc.StatusCode.UNAVAILABLE:
                if attempt < max_retries - 1:
                    # Exponential Backoff ê³„ì‚°
                    delay = min(base_delay * (2 ** attempt), max_delay)

                    # Jitter ì¶”ê°€ (ëœë¤ ìš”ì†Œ)
                    delay_with_jitter = delay + random.uniform(0, delay * 0.1)

                    logger.warning(
                        f"Server unavailable. "
                        f"Retry {attempt + 1}/{max_retries} in {delay_with_jitter:.2f}s..."
                    )
                    time.sleep(delay_with_jitter)
                    continue
                else:
                    logger.error("Max retries reached")
                    raise
            else:
                raise
```

---

# 6. ëª¨ë‹ˆí„°ë§

## 6.1 ìŠ¤íŠ¸ë¦¬ë° ë©”íŠ¸ë¦­

### StreamingMetrics í´ë˜ìŠ¤

```python
import time
from dataclasses import dataclass, field
from typing import List


@dataclass
class StreamingMetrics:
    """ìŠ¤íŠ¸ë¦¬ë° ì„±ëŠ¥ ë©”íŠ¸ë¦­"""

    start_time: float = field(default_factory=time.time)
    total_rows: int = 0
    total_chunks: int = 0
    total_bytes: int = 0

    chunk_latencies: List[float] = field(default_factory=list)

    def record_chunk(self, chunk_size: int, chunk_bytes: int, latency: float):
        """Chunk ì „ì†¡ ê¸°ë¡"""
        self.total_rows += chunk_size
        self.total_chunks += 1
        self.total_bytes += chunk_bytes
        self.chunk_latencies.append(latency)

    def get_summary(self) -> dict:
        """ìš”ì•½ í†µê³„"""
        elapsed = time.time() - self.start_time

        avg_latency = sum(self.chunk_latencies) / len(self.chunk_latencies) if self.chunk_latencies else 0
        throughput_rows = self.total_rows / elapsed if elapsed > 0 else 0
        throughput_mb = (self.total_bytes / 1024 / 1024) / elapsed if elapsed > 0 else 0

        return {
            'total_rows': self.total_rows,
            'total_chunks': self.total_chunks,
            'total_bytes': self.total_bytes,
            'elapsed_seconds': elapsed,
            'avg_chunk_latency_ms': avg_latency * 1000,
            'throughput_rows_per_sec': throughput_rows,
            'throughput_mb_per_sec': throughput_mb,
        }


# ì‚¬ìš© ì˜ˆì‹œ
def GetEmbeddings(self, request, context):
    """ë©”íŠ¸ë¦­ ì¶”ì """
    metrics = StreamingMetrics()

    df = load_data_from_pkl(Config.PKL_PATH)

    for chunk_df in chunker(df, chunk_size):
        chunk_start = time.time()
        chunk_bytes = 0

        for _, row in chunk_df.iterrows():
            embedding = create_embedding(row)
            chunk_bytes += embedding.ByteSize()
            yield embedding

        # Chunk ë©”íŠ¸ë¦­ ê¸°ë¡
        chunk_latency = time.time() - chunk_start
        metrics.record_chunk(len(chunk_df), chunk_bytes, chunk_latency)

        # ì£¼ê¸°ì ìœ¼ë¡œ ë©”íŠ¸ë¦­ ì¶œë ¥
        if metrics.total_chunks % 10 == 0:
            summary = metrics.get_summary()
            logger.info(
                f"Streaming progress: "
                f"{summary['total_rows']} rows, "
                f"{summary['throughput_rows_per_sec']:.0f} rows/sec"
            )

    # ìµœì¢… ìš”ì•½
    summary = metrics.get_summary()
    logger.info(f"Streaming completed: {summary}")
```

## 6.2 ë¡œê¹…

### êµ¬ì¡°í™”ëœ ë¡œê¹…

```python
import json
import logging


class StructuredLogger:
    """êµ¬ì¡°í™”ëœ JSON ë¡œê¹…"""

    def __init__(self, name: str):
        self.logger = logging.getLogger(name)

    def log_streaming_start(self, request):
        """ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ë¡œê·¸"""
        log_data = {
            'event': 'streaming_start',
            'last_processed_uuid': request.last_processed_uuid,
            'chunk_size': request.chunk_size,
            'timestamp': time.time(),
        }
        self.logger.info(json.dumps(log_data))

    def log_chunk_sent(self, chunk_num: int, chunk_size: int, total_sent: int):
        """Chunk ì „ì†¡ ë¡œê·¸"""
        log_data = {
            'event': 'chunk_sent',
            'chunk_num': chunk_num,
            'chunk_size': chunk_size,
            'total_sent': total_sent,
            'timestamp': time.time(),
        }
        self.logger.info(json.dumps(log_data))

    def log_streaming_end(self, total_rows: int, elapsed: float):
        """ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ë¡œê·¸"""
        log_data = {
            'event': 'streaming_end',
            'total_rows': total_rows,
            'elapsed_seconds': elapsed,
            'throughput': total_rows / elapsed if elapsed > 0 else 0,
            'timestamp': time.time(),
        }
        self.logger.info(json.dumps(log_data))


# ì‚¬ìš© ì˜ˆì‹œ
structured_logger = StructuredLogger(__name__)

def GetEmbeddings(self, request, context):
    structured_logger.log_streaming_start(request)

    start_time = time.time()
    total_sent = 0
    chunk_num = 0

    for chunk_df in chunker(df, chunk_size):
        chunk_num += 1

        for _, row in chunk_df.iterrows():
            yield create_embedding(row)
            total_sent += 1

        structured_logger.log_chunk_sent(chunk_num, len(chunk_df), total_sent)

    elapsed = time.time() - start_time
    structured_logger.log_streaming_end(total_sent, elapsed)
```

---

# 7. ìš”ì•½

## 7.1 í•µì‹¬ í¬ì¸íŠ¸

1. **Chunk ë¶„í• **
   - ê¶Œì¥ Chunk í¬ê¸°: 300 rows (~2MB)
   - ë™ì  ì¡°ì •ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ìƒí™©ì— ì ì‘

2. **Backpressure**
   - gRPCì˜ ìë™ Flow Control í™œìš©
   - ìˆ˜ë™ ì œì–´ë¡œ ë ˆì´í„´ì‹œ ì¡°ì ˆ ê°€ëŠ¥

3. **ì„±ëŠ¥ ìµœì í™”**
   - ë³‘ë ¬ ì²˜ë¦¬ (ThreadPoolExecutor)
   - ë°°ì¹˜ ì „ì†¡ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ)
   - ë©”ëª¨ë¦¬ í’€ (GC ì••ë°• ê°ì†Œ)

4. **ì—ëŸ¬ ë³µêµ¬**
   - Checkpoint ê¸°ë°˜ ì¬ì‹œì‘
   - Exponential Backoff ì¬ì‹œë„

5. **ëª¨ë‹ˆí„°ë§**
   - ìŠ¤íŠ¸ë¦¬ë° ë©”íŠ¸ë¦­ ì¶”ì 
   - êµ¬ì¡°í™”ëœ ë¡œê¹…

## 7.2 ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `chunker()` êµ¬í˜„ ì™„ë£Œ
- [ ] `calculate_optimal_chunk_size()` êµ¬í˜„ ì™„ë£Œ
- [ ] Backpressure ì²˜ë¦¬ í™•ì¸
- [ ] Checkpoint ê¸°ë°˜ ì¬ì‹œì‘ êµ¬í˜„ ì™„ë£Œ
- [ ] ë©”íŠ¸ë¦­ ë° ë¡œê¹… êµ¬í˜„ ì™„ë£Œ
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ

---

**ìµœì¢… ìˆ˜ì •ì¼:** 2025-12-11
