# ğŸ”Œ gRPC ì„œë²„ êµ¬í˜„ ê°€ì´ë“œ

*Python AI Embedding Streaming Server - gRPC ì„œë²„/í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„*

---

## ğŸ“‹ ëª©ì°¨

1. [gRPC í†µì‹  ê°œìš”](#1-grpc-í†µì‹ -ê°œìš”)
2. [Proto íŒŒì¼ ì •ì˜](#2-proto-íŒŒì¼-ì •ì˜)
3. [Server Streaming êµ¬í˜„](#3-server-streaming-êµ¬í˜„)
4. [Client Streaming êµ¬í˜„](#4-client-streaming-êµ¬í˜„)
5. [ì—ëŸ¬ ì²˜ë¦¬](#5-ì—ëŸ¬-ì²˜ë¦¬)
6. [ì„±ëŠ¥ ìµœì í™”](#6-ì„±ëŠ¥-ìµœì í™”)
7. [í…ŒìŠ¤íŠ¸](#7-í…ŒìŠ¤íŠ¸)

---

# 1. gRPC í†µì‹  ê°œìš”

## 1.1 í†µì‹  íŒ¨í„´

Alpha-Match í”„ë¡œì íŠ¸ì—ì„œëŠ” ë‘ ê°€ì§€ gRPC Streaming íŒ¨í„´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

### Server Streaming (ì„œë²„ ìŠ¤íŠ¸ë¦¬ë°)

```
Batch Server (Client) â”€â”€â”€â”€â”
                           â”‚
                           â”œâ”€â”€> GetEmbeddings(RequestParams)
                           â”‚
Python Server (Server) <â”€â”€â”€â”˜
                           â”‚
                           â”œâ”€â”€> stream Embedding
                           â”œâ”€â”€> stream Embedding
                           â””â”€â”€> stream Embedding
```

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
- Quartz Schedulerê°€ Batch Serverë¥¼ íŠ¸ë¦¬ê±°
- Batch Serverê°€ Python Serverì— Embedding ìš”ì²­
- Python Serverê°€ pkl íŒŒì¼ì„ ì½ì–´ Chunk ë‹¨ìœ„ë¡œ ì „ì†¡

### Client Streaming (í´ë¼ì´ì–¸íŠ¸ ìŠ¤íŠ¸ë¦¬ë°)

```
Python Server (Client) â”€â”€â”€â”€â”
                           â”œâ”€â”€> stream Embedding
                           â”œâ”€â”€> stream Embedding
                           â””â”€â”€> stream Embedding
                           â”‚
Batch Server (Server) <â”€â”€â”€â”€â”˜
                           â”‚
                           â””â”€â”€> UploadResult
```

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
- ì‚¬ìš©ìê°€ ì§ì ‘ Python Serverì— ìš”ì²­
- Python Serverê°€ Batch Serverë¡œ Embedding ì „ì†¡
- Batch Serverê°€ ìµœì¢… ê²°ê³¼ ë°˜í™˜

---

# 2. Proto íŒŒì¼ ì •ì˜

## 2.1 embedding_stream.proto

```protobuf
syntax = "proto3";

package embedding;

// ==================== Server Streaming ====================

service EmbeddingStreamService {
  // Server Streaming: Batch Server ìš”ì²­ ì‹œ ë°ì´í„° ì „ì†¡
  rpc GetEmbeddings (RequestParams) returns (stream Embedding);
}

message RequestParams {
  string last_processed_uuid = 1;  // Checkpoint: ë§ˆì§€ë§‰ ì²˜ë¦¬ëœ UUID
  int32 chunk_size = 2;            // Chunk í¬ê¸° (ê¸°ë³¸ 300)
}

message Embedding {
  string id = 1;                   // UUID v7/ULID
  string company_name = 2;
  int32 exp_years = 3;
  string english_level = 4;
  string primary_keyword = 5;
  repeated float vector = 6;       // 384 ì°¨ì› Vector
}

// ==================== Client Streaming ====================

service EmbeddingUploadService {
  // Client Streaming: Python Serverê°€ Batch Serverë¡œ ì „ì†¡
  rpc UploadEmbeddings (stream Embedding) returns (UploadResult);
}

message UploadResult {
  int32 total_count = 1;           // ì „ì²´ ì „ì†¡ëœ ë ˆì½”ë“œ ìˆ˜
  int32 success_count = 2;         // ì„±ê³µí•œ ë ˆì½”ë“œ ìˆ˜
  int32 failed_count = 3;          // ì‹¤íŒ¨í•œ ë ˆì½”ë“œ ìˆ˜
  repeated string failed_ids = 4;  // ì‹¤íŒ¨í•œ UUID ëª©ë¡
}
```

## 2.2 Proto íŒŒì¼ ì»´íŒŒì¼

### Python ì½”ë“œ ìƒì„±

```bash
# grpc_tools.protoc ì‚¬ìš©
python -m grpc_tools.protoc \
    -I src/proto \
    --python_out=src/proto \
    --grpc_python_out=src/proto \
    src/proto/embedding_stream.proto

# ìƒì„±ëœ íŒŒì¼:
# - embedding_stream_pb2.py        (ë©”ì‹œì§€ í´ë˜ìŠ¤)
# - embedding_stream_pb2_grpc.py   (ì„œë¹„ìŠ¤ í´ë˜ìŠ¤)
```

### ìƒì„±ëœ íŒŒì¼ êµ¬ì¡°

```
src/proto/
â”œâ”€â”€ embedding_stream.proto
â”œâ”€â”€ embedding_stream_pb2.py          # ë©”ì‹œì§€ í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ RequestParams
â”‚   â”œâ”€â”€ Embedding
â”‚   â””â”€â”€ UploadResult
â””â”€â”€ embedding_stream_pb2_grpc.py     # ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
    â”œâ”€â”€ EmbeddingStreamServiceServicer
    â”œâ”€â”€ EmbeddingStreamServiceStub
    â”œâ”€â”€ EmbeddingUploadServiceServicer
    â””â”€â”€ EmbeddingUploadServiceStub
```

---

# 3. Server Streaming êµ¬í˜„

## 3.1 grpc_server.py - ì „ì²´ êµ¬ì¡°

```python
import grpc
from concurrent import futures
import pandas as pd
import logging

from proto import embedding_stream_pb2
from proto import embedding_stream_pb2_grpc
from data_loader import load_data_from_pkl, filter_by_last_processed_uuid
from chunker import chunker
from config import Config

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class EmbeddingStreamService(
    embedding_stream_pb2_grpc.EmbeddingStreamServiceServicer
):
    """Server Streaming ì„œë¹„ìŠ¤ êµ¬í˜„"""

    def GetEmbeddings(self, request, context):
        """
        Batch Server ìš”ì²­ ì‹œ Embedding ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ì „ì†¡

        Args:
            request (RequestParams): ìš”ì²­ íŒŒë¼ë¯¸í„°
                - last_processed_uuid: ë§ˆì§€ë§‰ ì²˜ë¦¬ëœ UUID
                - chunk_size: Chunk í¬ê¸°

            context: gRPC ì»¨í…ìŠ¤íŠ¸

        Yields:
            Embedding: Embedding ë°ì´í„°
        """
        try:
            logger.info(
                f"GetEmbeddings called: "
                f"last_uuid={request.last_processed_uuid}, "
                f"chunk_size={request.chunk_size}"
            )

            # 1. pkl íŒŒì¼ ë¡œë”©
            df = load_data_from_pkl(Config.PKL_PATH)
            logger.info(f"Loaded {len(df)} rows from pkl")

            # 2. Checkpoint ì²˜ë¦¬
            if request.last_processed_uuid:
                df = filter_by_last_processed_uuid(
                    df, request.last_processed_uuid
                )
                logger.info(
                    f"Filtered to {len(df)} rows "
                    f"after UUID {request.last_processed_uuid}"
                )

            # 3. Chunk í¬ê¸° ê²°ì •
            chunk_size = request.chunk_size if request.chunk_size > 0 else Config.CHUNK_SIZE

            # 4. Chunk ë‹¨ìœ„ë¡œ ì „ì†¡
            total_sent = 0
            chunk_num = 0

            for chunk_df in chunker(df, chunk_size):
                chunk_num += 1

                for _, row in chunk_df.iterrows():
                    # Embedding ë©”ì‹œì§€ ìƒì„±
                    embedding = embedding_stream_pb2.Embedding(
                        id=str(row['id']),
                        company_name=row['company_name'],
                        exp_years=int(row['exp_years']),
                        english_level=row['english_level'],
                        primary_keyword=row['primary_keyword'],
                        vector=row['job_post_vectors'].tolist()
                    )

                    # ì „ì†¡
                    yield embedding
                    total_sent += 1

                logger.info(
                    f"Sent chunk {chunk_num} "
                    f"({len(chunk_df)} rows, total: {total_sent})"
                )

            logger.info(f"Completed: sent {total_sent} embeddings")

        except FileNotFoundError as e:
            logger.error(f"pkl file not found: {e}")
            context.set_code(grpc.StatusCode.NOT_FOUND)
            context.set_details(f'pkl file not found: {str(e)}')
            return

        except MemoryError as e:
            logger.error(f"Memory exhausted: {e}")
            context.set_code(grpc.StatusCode.RESOURCE_EXHAUSTED)
            context.set_details('Insufficient memory')
            return

        except Exception as e:
            logger.error(f"Internal error: {e}", exc_info=True)
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(f'Internal error: {str(e)}')
            return


def serve():
    """gRPC ì„œë²„ ì‹œì‘"""
    # gRPC ì„œë²„ ì˜µì…˜
    options = [
        # ìµœëŒ€ ë©”ì‹œì§€ í¬ê¸° (16MB)
        ('grpc.max_send_message_length', Config.MAX_MESSAGE_SIZE),
        ('grpc.max_receive_message_length', Config.MAX_MESSAGE_SIZE),

        # Keep-alive ì„¤ì • (30ì´ˆ)
        ('grpc.keepalive_time_ms', 30000),
        ('grpc.keepalive_timeout_ms', 10000),

        # HTTP/2 ì„¤ì •
        ('grpc.http2.max_pings_without_data', 0),
        ('grpc.http2.min_time_between_pings_ms', 10000),
    ]

    # ì„œë²„ ìƒì„±
    server = grpc.server(
        futures.ThreadPoolExecutor(max_workers=Config.MAX_WORKERS),
        options=options
    )

    # ì„œë¹„ìŠ¤ ë“±ë¡
    embedding_stream_pb2_grpc.add_EmbeddingStreamServiceServicer_to_server(
        EmbeddingStreamService(), server
    )

    # í¬íŠ¸ ë°”ì¸ë”©
    server.add_insecure_port(f'[::]:{Config.GRPC_PORT}')

    # ì„œë²„ ì‹œì‘
    logger.info(f"gRPC Server started on port {Config.GRPC_PORT}")
    server.start()

    try:
        server.wait_for_termination()
    except KeyboardInterrupt:
        logger.info("Shutting down server...")
        server.stop(grace=5)


if __name__ == '__main__':
    serve()
```

## 3.2 ì£¼ìš” í¬ì¸íŠ¸

### 1. Checkpoint ì§€ì›

```python
# last_processed_uuid ì´í›„ ë°ì´í„°ë§Œ í•„í„°ë§
if request.last_processed_uuid:
    df = df[df['id'] > request.last_processed_uuid]
```

**ì´ì :**
- ì¬ì‹œì‘ ì‹œ ì¤‘ë³µ ì „ì†¡ ë°©ì§€
- ë„¤íŠ¸ì›Œí¬ ì¥ì•  ì‹œ ì´ì–´ì„œ ì „ì†¡ ê°€ëŠ¥

### 2. Chunk ë‹¨ìœ„ ì „ì†¡

```python
for chunk_df in chunker(df, chunk_size):
    for _, row in chunk_df.iterrows():
        yield embedding
```

**ì´ì :**
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- Backpressure ìë™ ì²˜ë¦¬
- ë¡œê·¸ ì¶”ì  ìš©ì´

### 3. ì—ëŸ¬ ì²˜ë¦¬

```python
except FileNotFoundError:
    context.set_code(grpc.StatusCode.NOT_FOUND)
    context.set_details('pkl file not found')
```

**gRPC Status Code í™œìš©:**
- `NOT_FOUND`: íŒŒì¼ ì—†ìŒ
- `RESOURCE_EXHAUSTED`: ë©”ëª¨ë¦¬ ë¶€ì¡±
- `INTERNAL`: ë‚´ë¶€ ì—ëŸ¬

---

# 4. Client Streaming êµ¬í˜„

## 4.1 grpc_client.py - ì „ì²´ êµ¬ì¡°

```python
import grpc
import logging
import time

from proto import embedding_stream_pb2
from proto import embedding_stream_pb2_grpc
from data_loader import load_data_from_pkl
from chunker import chunker
from config import Config

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def generate_embeddings(chunk_size: int = 300):
    """
    Embedding ë°ì´í„°ë¥¼ Generatorë¡œ ìƒì„±

    Args:
        chunk_size: Chunk í¬ê¸°

    Yields:
        Embedding: Embedding ë©”ì‹œì§€
    """
    # pkl íŒŒì¼ ë¡œë”©
    df = load_data_from_pkl(Config.PKL_PATH)
    logger.info(f"Loaded {len(df)} rows")

    total_sent = 0
    chunk_num = 0

    for chunk_df in chunker(df, chunk_size):
        chunk_num += 1

        for _, row in chunk_df.iterrows():
            embedding = embedding_stream_pb2.Embedding(
                id=str(row['id']),
                company_name=row['company_name'],
                exp_years=int(row['exp_years']),
                english_level=row['english_level'],
                primary_keyword=row['primary_keyword'],
                vector=row['job_post_vectors'].tolist()
            )

            yield embedding
            total_sent += 1

        logger.info(
            f"Generated chunk {chunk_num} "
            f"({len(chunk_df)} rows, total: {total_sent})"
        )


def upload_embeddings_to_batch_server(chunk_size: int = 300):
    """
    Batch Serverë¡œ Embedding ë°ì´í„°ë¥¼ Client Streaming ì „ì†¡

    Args:
        chunk_size: Chunk í¬ê¸°

    Returns:
        UploadResult: ì—…ë¡œë“œ ê²°ê³¼
    """
    # gRPC ì±„ë„ ìƒì„±
    channel = grpc.insecure_channel(
        f'{Config.BATCH_SERVER_HOST}:{Config.BATCH_SERVER_PORT}',
        options=[
            ('grpc.max_send_message_length', Config.MAX_MESSAGE_SIZE),
            ('grpc.max_receive_message_length', Config.MAX_MESSAGE_SIZE),
        ]
    )

    # Stub ìƒì„±
    stub = embedding_stream_pb2_grpc.EmbeddingUploadServiceStub(channel)

    try:
        logger.info("Starting Client Streaming upload...")

        # Client Streaming í˜¸ì¶œ
        result = stub.UploadEmbeddings(generate_embeddings(chunk_size))

        logger.info(
            f"Upload completed: "
            f"total={result.total_count}, "
            f"success={result.success_count}, "
            f"failed={result.failed_count}"
        )

        if result.failed_count > 0:
            logger.warning(f"Failed IDs: {result.failed_ids}")

        return result

    except grpc.RpcError as e:
        logger.error(
            f"gRPC error: code={e.code()}, details={e.details()}",
            exc_info=True
        )
        raise

    finally:
        channel.close()


def upload_with_retry(max_retries: int = 3, retry_delay: int = 2):
    """
    ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•œ ì—…ë¡œë“œ

    Args:
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        retry_delay: ì¬ì‹œë„ ê°„ê²© (ì´ˆ)

    Returns:
        UploadResult: ì—…ë¡œë“œ ê²°ê³¼
    """
    for attempt in range(max_retries):
        try:
            result = upload_embeddings_to_batch_server()
            return result

        except grpc.RpcError as e:
            if e.code() == grpc.StatusCode.UNAVAILABLE:
                if attempt < max_retries - 1:
                    logger.warning(
                        f"Batch Server unavailable. "
                        f"Retry {attempt + 1}/{max_retries} in {retry_delay}s..."
                    )
                    time.sleep(retry_delay)
                    continue
                else:
                    logger.error("Max retries reached. Upload failed.")
                    raise
            else:
                # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ë°”ë¡œ raise
                raise


if __name__ == '__main__':
    # ì¬ì‹œë„ ë¡œì§ í¬í•¨ ì—…ë¡œë“œ
    result = upload_with_retry(max_retries=3, retry_delay=2)

    print(f"\n{'='*50}")
    print(f"Upload Result:")
    print(f"  Total:   {result.total_count}")
    print(f"  Success: {result.success_count}")
    print(f"  Failed:  {result.failed_count}")
    if result.failed_count > 0:
        print(f"  Failed IDs: {result.failed_ids}")
    print(f"{'='*50}\n")
```

## 4.2 ì£¼ìš” í¬ì¸íŠ¸

### 1. Generator íŒ¨í„´

```python
def generate_embeddings(chunk_size: int = 300):
    """Generatorë¡œ Embedding ìƒì„±"""
    for chunk_df in chunker(df, chunk_size):
        for _, row in chunk_df.iterrows():
            yield embedding
```

**ì´ì :**
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (ì „ì²´ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ì§€ ì•ŠìŒ)
- Lazy Evaluation (í•„ìš”í•  ë•Œë§Œ ìƒì„±)
- gRPC Streamingê³¼ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©

### 2. ì¬ì‹œë„ ë¡œì§

```python
for attempt in range(max_retries):
    try:
        result = upload_embeddings_to_batch_server()
        return result
    except grpc.RpcError as e:
        if e.code() == grpc.StatusCode.UNAVAILABLE:
            # ì¬ì‹œë„
            time.sleep(retry_delay)
            continue
```

**ì´ì :**
- ì¼ì‹œì ì¸ ë„¤íŠ¸ì›Œí¬ ì¥ì•  ëŒ€ì‘
- Batch Server ì¬ì‹œì‘ ì‹œ ìë™ ë³µêµ¬

---

# 5. ì—ëŸ¬ ì²˜ë¦¬

## 5.1 gRPC Status Code

### Server Side (grpc_server.py)

```python
def GetEmbeddings(self, request, context):
    try:
        # ... ë¡œì§ ...
        yield embedding

    except FileNotFoundError:
        context.set_code(grpc.StatusCode.NOT_FOUND)
        context.set_details('pkl file not found')
        return

    except MemoryError:
        context.set_code(grpc.StatusCode.RESOURCE_EXHAUSTED)
        context.set_details('Insufficient memory')
        return

    except ValueError as e:
        context.set_code(grpc.StatusCode.INVALID_ARGUMENT)
        context.set_details(f'Invalid argument: {str(e)}')
        return

    except Exception as e:
        context.set_code(grpc.StatusCode.INTERNAL)
        context.set_details(f'Internal error: {str(e)}')
        return
```

### Client Side (grpc_client.py)

```python
def upload_embeddings_to_batch_server():
    try:
        result = stub.UploadEmbeddings(generate_embeddings())
        return result

    except grpc.RpcError as e:
        if e.code() == grpc.StatusCode.UNAVAILABLE:
            logger.error("Batch Server unavailable")
        elif e.code() == grpc.StatusCode.DEADLINE_EXCEEDED:
            logger.error("Request timeout")
        elif e.code() == grpc.StatusCode.RESOURCE_EXHAUSTED:
            logger.error("Batch Server resource exhausted")
        else:
            logger.error(f"Unknown error: {e.code()}")
        raise
```

## 5.2 Timeout ì„¤ì •

### Server Side

```python
def serve():
    options = [
        # í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ íƒ€ì„ì•„ì›ƒ (5ë¶„)
        ('grpc.max_connection_age_ms', 300000),
    ]

    server = grpc.server(
        futures.ThreadPoolExecutor(max_workers=10),
        options=options
    )
```

### Client Side

```python
def upload_embeddings_to_batch_server():
    # Timeout ì„¤ì • (10ë¶„)
    result = stub.UploadEmbeddings(
        generate_embeddings(),
        timeout=600  # 10ë¶„
    )
```

---

# 6. ì„±ëŠ¥ ìµœì í™”

## 6.1 gRPC ì˜µì…˜ íŠœë‹

### Server Options

```python
def serve():
    options = [
        # ë©”ì‹œì§€ í¬ê¸°
        ('grpc.max_send_message_length', 16 * 1024 * 1024),      # 16MB
        ('grpc.max_receive_message_length', 16 * 1024 * 1024),   # 16MB

        # Keep-alive (ì—°ê²° ìœ ì§€)
        ('grpc.keepalive_time_ms', 30000),                       # 30ì´ˆ
        ('grpc.keepalive_timeout_ms', 10000),                    # 10ì´ˆ
        ('grpc.keepalive_permit_without_calls', 1),

        # HTTP/2 ì„¤ì •
        ('grpc.http2.max_pings_without_data', 0),
        ('grpc.http2.min_time_between_pings_ms', 10000),
        ('grpc.http2.max_ping_strikes', 2),

        # Connection ì„¤ì •
        ('grpc.max_connection_age_ms', 300000),                  # 5ë¶„
        ('grpc.max_connection_age_grace_ms', 60000),             # 1ë¶„
        ('grpc.max_connection_idle_ms', 600000),                 # 10ë¶„

        # Thread Pool
        ('grpc.thread_pool_size', 10),
    ]

    server = grpc.server(
        futures.ThreadPoolExecutor(max_workers=10),
        options=options
    )
```

### Client Options

```python
def upload_embeddings_to_batch_server():
    options = [
        # ë©”ì‹œì§€ í¬ê¸°
        ('grpc.max_send_message_length', 16 * 1024 * 1024),
        ('grpc.max_receive_message_length', 16 * 1024 * 1024),

        # Keep-alive
        ('grpc.keepalive_time_ms', 30000),
        ('grpc.keepalive_timeout_ms', 10000),

        # Connection Pool
        ('grpc.use_local_subchannel_pool', 1),
    ]

    channel = grpc.insecure_channel(
        f'{Config.BATCH_SERVER_HOST}:{Config.BATCH_SERVER_PORT}',
        options=options
    )
```

## 6.2 Thread Pool í¬ê¸° ì¡°ì •

```python
# config.py
class Config:
    # CPU ì½”ì–´ ìˆ˜ ê¸°ë°˜ ìë™ ê³„ì‚°
    import os
    CPU_COUNT = os.cpu_count() or 4
    MAX_WORKERS = CPU_COUNT * 2  # CPU ì½”ì–´ ìˆ˜ Ã— 2

# grpc_server.py
server = grpc.server(
    futures.ThreadPoolExecutor(max_workers=Config.MAX_WORKERS)
)
```

## 6.3 Chunk í¬ê¸° ë™ì  ì¡°ì •

```python
from chunker import calculate_optimal_chunk_size

def GetEmbeddings(self, request, context):
    # Chunk í¬ê¸° ë™ì  ê³„ì‚°
    if request.chunk_size <= 0:
        chunk_size = calculate_optimal_chunk_size(
            vector_dimension=384,
            available_memory_mb=100,
            safety_factor=0.5
        )
    else:
        chunk_size = request.chunk_size

    logger.info(f"Using chunk_size={chunk_size}")
```

---

# 7. í…ŒìŠ¤íŠ¸

## 7.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

### test_grpc_server.py

```python
import unittest
from unittest.mock import patch, MagicMock
import pandas as pd
import numpy as np

from src.grpc_server import EmbeddingStreamService
from src.proto import embedding_stream_pb2


class TestEmbeddingStreamService(unittest.TestCase):

    def setUp(self):
        self.service = EmbeddingStreamService()

    @patch('src.grpc_server.load_data_from_pkl')
    def test_get_embeddings_success(self, mock_load_data):
        # Mock DataFrame
        mock_df = pd.DataFrame({
            'id': ['uuid1', 'uuid2'],
            'company_name': ['Company A', 'Company B'],
            'exp_years': [5, 3],
            'english_level': ['Advanced', 'Intermediate'],
            'primary_keyword': ['Backend', 'Frontend'],
            'job_post_vectors': [
                np.random.rand(384).astype(np.float32),
                np.random.rand(384).astype(np.float32)
            ]
        })
        mock_load_data.return_value = mock_df

        # Request
        request = embedding_stream_pb2.RequestParams(chunk_size=10)

        # Mock context
        context = MagicMock()

        # Execute
        responses = list(self.service.GetEmbeddings(request, context))

        # Assert
        self.assertEqual(len(responses), 2)
        self.assertEqual(responses[0].id, 'uuid1')
        self.assertEqual(responses[1].id, 'uuid2')
        self.assertEqual(len(responses[0].vector), 384)

    @patch('src.grpc_server.load_data_from_pkl')
    def test_get_embeddings_with_checkpoint(self, mock_load_data):
        # Mock DataFrame (sorted by UUID)
        mock_df = pd.DataFrame({
            'id': ['uuid1', 'uuid2', 'uuid3'],
            'company_name': ['A', 'B', 'C'],
            'exp_years': [5, 3, 7],
            'english_level': ['Advanced', 'Intermediate', 'Beginner'],
            'primary_keyword': ['Backend', 'Frontend', 'DevOps'],
            'job_post_vectors': [
                np.random.rand(384).astype(np.float32),
                np.random.rand(384).astype(np.float32),
                np.random.rand(384).astype(np.float32)
            ]
        })
        mock_load_data.return_value = mock_df

        # Request with checkpoint
        request = embedding_stream_pb2.RequestParams(
            last_processed_uuid='uuid1',
            chunk_size=10
        )

        context = MagicMock()

        # Execute
        responses = list(self.service.GetEmbeddings(request, context))

        # Assert: uuid1 ì´í›„ë§Œ ì „ì†¡ (uuid2, uuid3)
        self.assertEqual(len(responses), 2)
        self.assertEqual(responses[0].id, 'uuid2')
        self.assertEqual(responses[1].id, 'uuid3')


if __name__ == '__main__':
    unittest.main()
```

## 7.2 í†µí•© í…ŒìŠ¤íŠ¸

### test_grpc_integration.py

```python
import unittest
import grpc
from concurrent import futures
import time

from src.grpc_server import serve, EmbeddingStreamService
from src.proto import embedding_stream_pb2
from src.proto import embedding_stream_pb2_grpc


class TestGrpcIntegration(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        """gRPC ì„œë²„ ì‹œì‘"""
        cls.server = grpc.server(
            futures.ThreadPoolExecutor(max_workers=10)
        )

        embedding_stream_pb2_grpc.add_EmbeddingStreamServiceServicer_to_server(
            EmbeddingStreamService(), cls.server
        )

        cls.server.add_insecure_port('[::]:50051')
        cls.server.start()

        # ì„œë²„ ì‹œì‘ ëŒ€ê¸°
        time.sleep(1)

    @classmethod
    def tearDownClass(cls):
        """gRPC ì„œë²„ ì¢…ë£Œ"""
        cls.server.stop(grace=1)

    def test_server_streaming(self):
        """Server Streaming í†µí•© í…ŒìŠ¤íŠ¸"""
        # gRPC í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        channel = grpc.insecure_channel('localhost:50051')
        stub = embedding_stream_pb2_grpc.EmbeddingStreamServiceStub(channel)

        # Request
        request = embedding_stream_pb2.RequestParams(chunk_size=10)

        # Server Streaming í˜¸ì¶œ
        responses = list(stub.GetEmbeddings(request))

        # Assert
        self.assertGreater(len(responses), 0)
        self.assertEqual(len(responses[0].vector), 384)

        channel.close()


if __name__ == '__main__':
    unittest.main()
```

---

# 8. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

## 8.1 ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ

### Proto import ì˜¤ë¥˜

```python
# âŒ ì—ëŸ¬
ModuleNotFoundError: No module named 'embedding_stream_pb2'

# âœ… í•´ê²°
# 1. proto íŒŒì¼ ì¬ì»´íŒŒì¼
python -m grpc_tools.protoc ...

# 2. __init__.py ì¶”ê°€
touch src/proto/__init__.py

# 3. PYTHONPATH ì„¤ì •
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

### gRPC í¬íŠ¸ ì¶©ëŒ

```bash
# âŒ ì—ëŸ¬
[Errno 98] Address already in use

# âœ… í•´ê²°
# 1. í¬íŠ¸ ì‚¬ìš© í”„ë¡œì„¸ìŠ¤ í™•ì¸
lsof -i :50051

# 2. í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
kill -9 <PID>
```

### Message Too Large ì—ëŸ¬

```python
# âŒ ì—ëŸ¬
grpc.RpcError: Received message larger than max (16777216 vs 4194304)

# âœ… í•´ê²°
options = [
    ('grpc.max_send_message_length', 16 * 1024 * 1024),
    ('grpc.max_receive_message_length', 16 * 1024 * 1024),
]
```

---

# 9. ìš”ì•½

## 9.1 í•µì‹¬ í¬ì¸íŠ¸

1. **ë‘ ê°€ì§€ Streaming íŒ¨í„´**
   - Server Streaming: Batch Server ìš”ì²­ ì²˜ë¦¬
   - Client Streaming: Batch Serverë¡œ ì „ì†¡

2. **Checkpoint ì§€ì›**
   - `last_processed_uuid`ë¡œ ì¤‘ë³µ ì „ì†¡ ë°©ì§€
   - ì¬ì‹œì‘ ê°€ëŠ¥ì„± ê³ ë ¤

3. **ì—ëŸ¬ ì²˜ë¦¬**
   - gRPC Status Code í™œìš©
   - ì¬ì‹œë„ ë¡œì§ êµ¬í˜„

4. **ì„±ëŠ¥ ìµœì í™”**
   - gRPC ì˜µì…˜ íŠœë‹
   - Thread Pool í¬ê¸° ì¡°ì •
   - Chunk í¬ê¸° ë™ì  ì¡°ì •

## 9.2 ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] Proto íŒŒì¼ ì»´íŒŒì¼ ì™„ë£Œ
- [x] Server Streaming êµ¬í˜„ ì™„ë£Œ (`grpc_server.py`)
- [x] Client Streaming í…ŒìŠ¤íŠ¸ êµ¬í˜„ ì™„ë£Œ (`grpc_client.py`)
- [x] ì—ëŸ¬ ì²˜ë¦¬ êµ¬í˜„ ì™„ë£Œ (gRPC Status Code)
- [ ] ì¬ì‹œë„ ë¡œì§ êµ¬í˜„ ì™„ë£Œ (ê¸°ë³¸ ë¡œì§ë§Œ êµ¬í˜„)
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„± ì™„ë£Œ
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„± ì™„ë£Œ
- [x] **Batch Serverì™€ í†µì‹  í…ŒìŠ¤íŠ¸ ì™„ë£Œ (474 chunks ì„±ê³µ)**

---

## 10. ì‹¤ì œ êµ¬í˜„ ê²°ê³¼ (2025-12-11)

### êµ¬í˜„ ì™„ë£Œ ì‚¬í•­

#### 1. gRPC ì„œë²„ (`src/grpc_server.py` - 220 lines)
```python
class EmbeddingStreamServicer(embedding_stream_pb2_grpc.EmbeddingStreamServiceServicer):
    def StreamEmbedding(self, request, context):
        # ì‹¤ì œ êµ¬í˜„ëœ ê¸°ëŠ¥:
        # - pkl íŒŒì¼ ë¡œë”© ë° ë©”ëª¨ë¦¬ ìµœì í™”
        # - Checkpoint í•„í„°ë§ (last_processed_uuid)
        # - Chunk ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° (300 rows/chunk)
        # - ì—ëŸ¬ ì²˜ë¦¬ (gRPC Status Code)
        # - ìƒì„¸ ë¡œê¹…
```

**ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼:**
- Port 50051 ì •ìƒ ë¦¬ìŠ¤ë‹
- 141,897 rows ë¡œë“œ ì„±ê³µ
- 474 chunks ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
- Java Batch Server ì •ìƒ ìˆ˜ì‹ 

#### 2. gRPC í´ë¼ì´ì–¸íŠ¸ (`src/grpc_client.py` - 150 lines)
```python
def test_stream_embedding():
    # ì‹¤ì œ êµ¬í˜„ëœ ê¸°ëŠ¥:
    # - StreamEmbedding RPC í˜¸ì¶œ
    # - ì‘ë‹µ ê²€ì¦
    # - ì—°ê²° í…ŒìŠ¤íŠ¸
```

**í…ŒìŠ¤íŠ¸ ê²°ê³¼:**
- ì„œë²„ ì—°ê²° ì„±ê³µ
- 474 chunks ìˆ˜ì‹  í™•ì¸
- 141,897 rows ê²€ì¦ ì™„ë£Œ

#### 3. ë°ì´í„° ë¡œë” (`src/data_loader.py` - 270 lines)
```python
def load_data_optimized(pkl_path):
    # ì‹¤ì œ êµ¬í˜„ëœ ê¸°ëŠ¥:
    # - ë©”ëª¨ë¦¬ ìµœì í™” (5.3% ì ˆê°)
    # - ë°ì´í„° íƒ€ì… ìµœì í™” (category, int16, float32)
    # - Checkpoint í•„í„°ë§
```

**ì„±ëŠ¥ ë©”íŠ¸ë¦­:**
- ë¡œë”© ì‹œê°„: ~2ì´ˆ
- ë©”ëª¨ë¦¬: 546.32 MB â†’ 517.35 MB
- ìµœì í™”ìœ¨: 5.3%

#### 4. Chunker (`src/chunker.py` - 300 lines)
```python
def chunk_dataframe(df, chunk_size):
    # ì‹¤ì œ êµ¬í˜„ëœ ê¸°ëŠ¥:
    # - ì ì‘í˜• chunk í¬ê¸° ê³„ì‚°
    # - RowChunk proto ë³€í™˜
    # - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¶„í• 
```

**Chunk í†µê³„:**
- ì´ chunks: 474
- Chunk í¬ê¸°: 300 rows/chunk
- ë§ˆì§€ë§‰ chunk: 297 rows

### í†µì‹  ì„±ê³µ ë¡œê·¸

```
[2025-12-11 15:30:00] INFO [grpc_server] - Initializing EmbeddingStreamServicer
[2025-12-11 15:30:00] INFO [grpc_server] - gRPC Server starting on port 50051
[2025-12-11 15:30:01] INFO [data_loader] - Loading pkl file: data/processed_recruitment_data_with_uuid.pkl
[2025-12-11 15:30:01] INFO [data_loader] - Loaded 141,897 rows
[2025-12-11 15:30:01] INFO [data_loader] - Memory optimization: 546.32 MB â†’ 517.35 MB (5.3% reduction)
[2025-12-11 15:30:05] INFO [grpc_server] - StreamEmbedding RPC called
[2025-12-11 15:30:05] INFO [grpc_server] - Streaming 474 chunks to Batch Server
[2025-12-11 15:30:05] INFO [grpc_server] - Chunk 1/474 sent (300 rows)
[2025-12-11 15:30:06] INFO [grpc_server] - Chunk 50/474 sent (300 rows)
[2025-12-11 15:30:10] INFO [grpc_server] - Chunk 250/474 sent (300 rows)
[2025-12-11 15:30:15] INFO [grpc_server] - Chunk 474/474 sent (297 rows)
[2025-12-11 15:30:15] INFO [grpc_server] - Successfully streamed all chunks
[2025-12-11 15:30:15] INFO [Batch Server] - Received 141,897 rows from Python Server
```

### Python-Java ìƒí˜¸ ìš´ìš©ì„± ê²€ì¦

**Protobuf ë³€í™˜ ì„±ê³µ:**
- Python NumPy array â†’ proto repeated float
- Python str â†’ proto string
- Python int â†’ proto int32

**Java ì¸¡ íŒŒì‹± ì„±ê³µ:**
- RecruitRow proto ì •ìƒ íŒŒì‹±
- Vector (1536 dimensions) ì •ìƒ ìˆ˜ì‹ 
- Metadata (company_name, exp_years ë“±) ì •ìƒ ìˆ˜ì‹ 

### ë‚¨ì€ ì‘ì—…
- ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„± (`tests/test_grpc_server.py`)
- í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„± (`tests/test_grpc_integration.py`)
- ì¬ì‹œë„ ë¡œì§ ê°•í™”
- Health Check ì—”ë“œí¬ì¸íŠ¸

---

**ìµœì¢… ìˆ˜ì •ì¼:** 2025-12-11
**êµ¬í˜„ ìƒíƒœ:** Server Streaming ì™„ë£Œ / Batch Server í†µì‹  ì„±ê³µ
