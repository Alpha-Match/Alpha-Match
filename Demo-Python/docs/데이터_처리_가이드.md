# ë°ì´í„° ì²˜ë¦¬ ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2025-12-17
**í”„ë¡œì íŠ¸**: Alpha-Match Demo-Python Server
**ëŒ€ìƒ**: Chunk Loader, ë„ë©”ì¸ ëª¨ë¸, ë°ì´í„° ê²€ì¦

---

## ğŸ“‹ ëª©ì°¨

1. [Chunk Loader ê°œìš”](#1-chunk-loader-ê°œìš”)
2. [3ê°€ì§€ Loader êµ¬í˜„](#2-3ê°€ì§€-loader-êµ¬í˜„)
3. [2-Tier Registry íŒ¨í„´](#3-2-tier-registry-íŒ¨í„´)
4. [ë„ë©”ì¸ ëª¨ë¸](#4-ë„ë©”ì¸-ëª¨ë¸)
5. [ë°ì´í„° ê²€ì¦](#5-ë°ì´í„°-ê²€ì¦)
6. [ë©”ëª¨ë¦¬ ìµœì í™”](#6-ë©”ëª¨ë¦¬-ìµœì í™”)

---

## 1. Chunk Loader ê°œìš”

### 1.1 ëª©ì 

ëŒ€ìš©ëŸ‰ ë°ì´í„° íŒŒì¼(pkl, csv, parquet)ì„ **ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬**í•˜ê¸° ìœ„í•´ Iterator íŒ¨í„´ ê¸°ë°˜ Chunk Loaderë¥¼ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

### 1.2 í•µì‹¬ ê°œë…

```python
# âŒ Bad: ì „ì²´ ë¡œë”© (ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ„í—˜)
df = pd.read_pickle('large_file.pkl')  # 500MB ì „ì²´ ë¡œë”©
process_all(df)  # OOM ê°€ëŠ¥ì„±

# âœ… Good: Chunk ë‹¨ìœ„ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
loader = PklChunkLoader(domain="recruit")
for chunk in loader.load_chunks('large_file.pkl', chunk_size=1000):
    process_chunk(chunk)  # 1000 rowsì”©ë§Œ ë©”ëª¨ë¦¬ì— ì ì¬
```

### 1.3 Iterator íŒ¨í„´ì˜ ì´ì 

1. **ë©”ëª¨ë¦¬ íš¨ìœ¨**: ì „ì²´ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ì§€ ì•ŠìŒ
2. **ì ì§„ì  ì²˜ë¦¬**: í•„ìš”í•œ ë§Œí¼ë§Œ ë¡œë”© (Lazy Evaluation)
3. **Backpressure ì§€ì›**: gRPC Streamingê³¼ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©
4. **ì¡°ê¸° ì¢…ë£Œ ê°€ëŠ¥**: í•„ìš”í•œ ë§Œí¼ë§Œ ì²˜ë¦¬ í›„ ì¤‘ë‹¨ ê°€ëŠ¥

---

## 2. 3ê°€ì§€ Loader êµ¬í˜„

### 2.1 BaseChunkLoader (ì¶”ìƒ í´ë˜ìŠ¤)

#### Protocol ê¸°ë°˜ ì œë„¤ë¦­

```python
from typing import Iterator, List, TypeVar, Protocol, Generic
from abc import ABC, abstractmethod

T_Row = TypeVar('T_Row', covariant=True)

class BaseChunkLoader(ABC, Generic[T_Row]):
    """Chunk Loader ì¶”ìƒ í´ë˜ìŠ¤"""

    def __init__(self, domain: str):
        self.domain = domain

    @abstractmethod
    def load_chunks(
        self,
        file_path: str,
        chunk_size: int = 1000
    ) -> Iterator[List[T_Row]]:
        """íŒŒì¼ì„ Chunk ë‹¨ìœ„ë¡œ ë¡œë”© (Iterator íŒ¨í„´)"""
        pass

    @abstractmethod
    def _parse_row(self, row_data: Any) -> T_Row:
        """ë‹¨ì¼ Rowë¥¼ ë„ë©”ì¸ ëª¨ë¸ë¡œ ë³€í™˜"""
        pass
```

#### TypeVar ê³µë³€ì„± (covariant=True)

- `T_Row`ëŠ” ê³µë³€ì (covariant) íƒ€ì… ë³€ìˆ˜
- `RecruitData`, `CandidateData` ë“± êµ¬ì²´ì ì¸ íƒ€ì…ìœ¼ë¡œ ì¹˜í™˜ ê°€ëŠ¥
- Batch-Serverì˜ `DataProcessor<T>` íŒ¨í„´ê³¼ ë§¤í•‘

### 2.2 PklChunkLoader (Pickle íŒŒì¼)

#### êµ¬í˜„ ë°©ì‹

```python
class PklChunkLoader(BaseChunkLoader[T_Row]):
    """Pickle íŒŒì¼ Chunk Loader"""

    def load_chunks(
        self,
        file_path: str,
        chunk_size: int = 1000
    ) -> Iterator[List[T_Row]]:
        """Pandas read_pickle + iloc slicing"""

        # 1. ì „ì²´ DataFrame ë¡œë“œ (ë¶ˆê°€í”¼)
        df = pd.read_pickle(file_path)

        # 2. Chunk ë‹¨ìœ„ ìŠ¬ë¼ì´ì‹±
        total_rows = len(df)
        for start in range(0, total_rows, chunk_size):
            end = min(start + chunk_size, total_rows)
            chunk_df = df.iloc[start:end]

            # 3. ë„ë©”ì¸ ëª¨ë¸ë¡œ ë³€í™˜
            chunk_data = []
            for _, row in chunk_df.iterrows():
                domain_data = self._parse_row(row)
                chunk_data.append(domain_data)

            yield chunk_data

    def _parse_row(self, row: pd.Series) -> T_Row:
        """Pandas Series â†’ ë„ë©”ì¸ ëª¨ë¸"""
        return self.domain_model_class(
            id=str(row['id']),
            company_name=row['company_name'],
            exp_years=int(row['exp_years']),
            english_level=row['english_level'],
            primary_keyword=row['primary_keyword'],
            vector=row['job_post_vectors'].tolist()
        )
```

#### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

| Chunk Size | ë©”ëª¨ë¦¬ ì‚¬ìš© (384d vector) |
|-----------|-------------------------|
| 100 | ~650 KB |
| 1000 | ~6.5 MB |
| 10000 | ~65 MB |

### 2.3 CsvChunkLoader (CSV íŒŒì¼)

#### êµ¬í˜„ ë°©ì‹

```python
class CsvChunkLoader(BaseChunkLoader[T_Row]):
    """CSV íŒŒì¼ Chunk Loader"""

    def load_chunks(
        self,
        file_path: str,
        chunk_size: int = 1000
    ) -> Iterator[List[T_Row]]:
        """Pandas read_csv(chunksize) ì‚¬ìš©"""

        # 1. Chunk Iterator ìƒì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        chunk_iter = pd.read_csv(
            file_path,
            chunksize=chunk_size,
            dtype={
                'id': str,
                'company_name': 'category',
                'exp_years': np.int16,
                'english_level': 'category',
                'primary_keyword': 'category'
            }
        )

        # 2. Chunk ì²˜ë¦¬
        for chunk_df in chunk_iter:
            chunk_data = []
            for _, row in chunk_df.iterrows():
                domain_data = self._parse_row(row)
                chunk_data.append(domain_data)

            yield chunk_data

    def _parse_row(self, row: pd.Series) -> T_Row:
        """Pandas Series â†’ ë„ë©”ì¸ ëª¨ë¸ (Vector íŒŒì‹±)"""
        return self.domain_model_class(
            id=str(row['id']),
            company_name=row['company_name'],
            exp_years=int(row['exp_years']),
            english_level=row['english_level'],
            primary_keyword=row['primary_keyword'],
            vector=self._parse_vector(row['vector'])  # ë¬¸ìì—´ â†’ List[float]
        )

    def _parse_vector(self, vector_str: str) -> List[float]:
        """CSV ë¬¸ìì—´ â†’ Python List"""
        try:
            # JSON í˜•íƒœ: "[0.1, 0.2, 0.3]"
            return json.loads(vector_str)
        except json.JSONDecodeError:
            # ê³µë°± ë¶„ë¦¬: "0.1 0.2 0.3"
            return [float(x) for x in vector_str.split()]

    def _parse_array(self, array_str: str) -> List[str]:
        """CSV ë¬¸ìì—´ â†’ Python List (skills ë°°ì—´)"""
        try:
            # JSON í˜•íƒœ: ["Python", "Java"]
            return json.loads(array_str)
        except json.JSONDecodeError:
            # ì‰¼í‘œ ë¶„ë¦¬: "Python,Java,C++"
            return [x.strip() for x in array_str.split(',')]
```

#### Vector íŒŒì‹± ì˜ˆì‹œ

```python
# Case 1: JSON í˜•íƒœ
"[0.123, 0.456, 0.789]"  â†’ [0.123, 0.456, 0.789]

# Case 2: ê³µë°± ë¶„ë¦¬
"0.123 0.456 0.789"  â†’ [0.123, 0.456, 0.789]

# Case 3: Array
"[\"Python\", \"Java\"]"  â†’ ["Python", "Java"]
```

### 2.4 ParquetChunkLoader (Parquet íŒŒì¼)

#### êµ¬í˜„ ë°©ì‹

```python
import pyarrow.parquet as pq

class ParquetChunkLoader(BaseChunkLoader[T_Row]):
    """Parquet íŒŒì¼ Chunk Loader"""

    def load_chunks(
        self,
        file_path: str,
        chunk_size: int = 1000
    ) -> Iterator[List[T_Row]]:
        """PyArrow iter_batches ì‚¬ìš©"""

        # 1. Parquet íŒŒì¼ ì—´ê¸°
        parquet_file = pq.ParquetFile(file_path)

        # 2. Batch Iterator ìƒì„±
        for batch in parquet_file.iter_batches(batch_size=chunk_size):
            # 3. PyArrow Table â†’ Pandas DataFrame
            chunk_df = batch.to_pandas()

            # 4. ë„ë©”ì¸ ëª¨ë¸ë¡œ ë³€í™˜
            chunk_data = []
            for _, row in chunk_df.iterrows():
                domain_data = self._parse_row(row)
                chunk_data.append(domain_data)

            yield chunk_data

    def _parse_row(self, row: pd.Series) -> T_Row:
        """Pandas Series â†’ ë„ë©”ì¸ ëª¨ë¸"""
        # Parquetì€ Native Array íƒ€ì… ì§€ì› (ë³€í™˜ ë¶ˆí•„ìš”)
        return self.domain_model_class(
            id=str(row['id']),
            company_name=row['company_name'],
            exp_years=int(row['exp_years']),
            english_level=row['english_level'],
            primary_keyword=row['primary_keyword'],
            vector=row['vector'].tolist()  # NumPy array â†’ List
        )
```

#### Parquetì˜ ì´ì 

- **ì»¬ëŸ¼ ê¸°ë°˜ ì €ì¥**: í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì½ê¸° ê°€ëŠ¥
- **ì••ì¶•ë¥  ë†’ìŒ**: CSV ëŒ€ë¹„ 70-80% ì‘ìŒ
- **ë„¤ì´í‹°ë¸Œ Array íƒ€ì…**: Vector/Arrayë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥
- **ìŠ¤í‚¤ë§ˆ í¬í•¨**: ë°ì´í„° íƒ€ì… ì •ë³´ ìë™ ë³´ì¡´

---

## 3. 2-Tier Registry íŒ¨í„´

### 3.1 ê°œë…

`(domain, format)` íŠœí”Œì„ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ Loader í´ë˜ìŠ¤ë¥¼ ë§¤í•‘í•©ë‹ˆë‹¤.

### 3.2 êµ¬í˜„

```python
from typing import Dict, Tuple, Type
from enum import Enum

class DataFormat(str, Enum):
    """ì§€ì› ë°ì´í„° í¬ë§·"""
    PKL = "pkl"
    CSV = "csv"
    PARQUET = "parquet"

# Registry: (domain, format) â†’ Loader Class
_loader_class_registry: Dict[Tuple[str, DataFormat], Type[BaseChunkLoader]] = {
    # Recruit ë„ë©”ì¸
    ("recruit", DataFormat.PKL): PklChunkLoader,
    ("recruit", DataFormat.CSV): CsvChunkLoader,
    ("recruit", DataFormat.PARQUET): ParquetChunkLoader,

    # Candidate ë„ë©”ì¸
    ("candidate", DataFormat.PKL): PklChunkLoader,
    ("candidate", DataFormat.CSV): CsvChunkLoader,
    ("candidate", DataFormat.PARQUET): ParquetChunkLoader,

    # SkillEmbeddingDic ë„ë©”ì¸
    ("skill_embedding_dic", DataFormat.PKL): PklChunkLoader,
    ("skill_embedding_dic", DataFormat.CSV): CsvChunkLoader,
}
```

### 3.3 Factory í•¨ìˆ˜

#### ëª…ì‹œì  ë¡œë” íšë“

```python
def get_loader(domain: str, format: DataFormat) -> BaseChunkLoader:
    """ëª…ì‹œì  ë¡œë” íšë“"""
    key = (domain, format)

    if key not in _loader_class_registry:
        raise ValueError(f"Unsupported (domain={domain}, format={format})")

    loader_class = _loader_class_registry[key]
    return loader_class(domain=domain)

# ì‚¬ìš© ì˜ˆì‹œ
loader = get_loader("recruit", DataFormat.PKL)
```

#### ìë™ ê°ì§€ (íŒŒì¼ í™•ì¥ì ê¸°ë°˜)

```python
def get_loader_auto(domain: str, file_path: str) -> BaseChunkLoader:
    """íŒŒì¼ í™•ì¥ì ê¸°ë°˜ ìë™ í¬ë§· ê°ì§€"""
    import os

    # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
    ext = os.path.splitext(file_path)[1].lower()

    # í™•ì¥ì â†’ í¬ë§· ë§¤í•‘
    format_map = {
        '.pkl': DataFormat.PKL,
        '.csv': DataFormat.CSV,
        '.parquet': DataFormat.PARQUET,
        '.pq': DataFormat.PARQUET,
    }

    if ext not in format_map:
        raise ValueError(f"Unknown file extension: {ext}")

    format = format_map[ext]
    return get_loader(domain, format)

# ì‚¬ìš© ì˜ˆì‹œ
loader = get_loader_auto("recruit", "recruit_embeddings.pkl")  # PklChunkLoader
loader = get_loader_auto("candidate", "candidate_data.csv")    # CsvChunkLoader
```

---

## 4. ë„ë©”ì¸ ëª¨ë¸

### 4.1 RecruitData (384ì°¨ì›)

```python
from dataclasses import dataclass
from pydantic import field_validator

@dataclass
class RecruitData:
    """Recruit ë„ë©”ì¸ ëª¨ë¸"""
    id: str                   # UUID v7
    company_name: str         # íšŒì‚¬ëª…
    exp_years: int            # ê²½ë ¥ (ì—°)
    english_level: str        # ì˜ì–´ ë ˆë²¨
    primary_keyword: str      # ì£¼ìš” í‚¤ì›Œë“œ
    vector: List[float]       # 384ì°¨ì› ë²¡í„°

    @field_validator('vector')
    def validate_vector_dimension(cls, v):
        """ë²¡í„° ì°¨ì› ê²€ì¦"""
        if len(v) != 384:
            raise ValueError(f"Expected 384d vector, got {len(v)}d")
        return v

    @field_validator('exp_years')
    def validate_exp_years(cls, v):
        """ê²½ë ¥ ê²€ì¦"""
        if v < 0:
            raise ValueError(f"exp_years must be >= 0, got {v}")
        return v
```

### 4.2 CandidateData (768ì°¨ì›)

```python
@dataclass
class CandidateData:
    """Candidate ë„ë©”ì¸ ëª¨ë¸"""
    candidate_id: str          # UUID v7
    position_category: str     # ì§ì¢… ì¹´í…Œê³ ë¦¬
    experience_years: int      # ê²½ë ¥ (ì—°)
    original_resume: str       # ì›ë¬¸ ì´ë ¥ì„œ
    skills: List[str]          # ë³´ìœ  ìŠ¤í‚¬ ë°°ì—´
    vector: List[float]        # 768ì°¨ì› ë²¡í„°

    @field_validator('vector')
    def validate_vector_dimension(cls, v):
        if len(v) != 768:
            raise ValueError(f"Expected 768d vector, got {len(v)}d")
        return v

    @field_validator('skills')
    def validate_skills(cls, v):
        """ìŠ¤í‚¬ ë°°ì—´ ê²€ì¦"""
        if not v or len(v) == 0:
            raise ValueError("skills array must not be empty")
        return v

    @field_validator('experience_years')
    def validate_experience_years(cls, v):
        if v < 0:
            raise ValueError(f"experience_years must be >= 0, got {v}")
        return v
```

### 4.3 SkillEmbeddingDicData (768ì°¨ì›)

```python
@dataclass
class SkillEmbeddingDicData:
    """SkillEmbeddingDic ë„ë©”ì¸ ëª¨ë¸"""
    skill: str                 # ìŠ¤í‚¬ëª… (PK)
    position_category: str     # ì§ì¢… ì¹´í…Œê³ ë¦¬
    vector: List[float]        # 768ì°¨ì› ë²¡í„°

    @field_validator('vector')
    def validate_vector_dimension(cls, v):
        if len(v) != 768:
            raise ValueError(f"Expected 768d vector, got {len(v)}d")
        return v

    @field_validator('skill')
    def validate_skill(cls, v):
        """ìŠ¤í‚¬ëª… ê²€ì¦"""
        if not v or len(v.strip()) == 0:
            raise ValueError("skill must not be empty")
        return v.strip()
```

### 4.4 ë„ë©”ì¸ ì„¤ì •

```python
from typing import NamedTuple, Type

class DomainConfig(NamedTuple):
    """ë„ë©”ì¸ ì„¤ì •"""
    expected_vector_dim: int
    model_class: Type

# ë„ë©”ì¸ë³„ ì„¤ì •
_domain_config = {
    "recruit": DomainConfig(
        expected_vector_dim=384,
        model_class=RecruitData
    ),
    "candidate": DomainConfig(
        expected_vector_dim=768,
        model_class=CandidateData
    ),
    "skill_embedding_dic": DomainConfig(
        expected_vector_dim=768,
        model_class=SkillEmbeddingDicData
    ),
}

def get_domain_config(domain: str) -> DomainConfig:
    """ë„ë©”ì¸ ì„¤ì • íšë“"""
    if domain not in _domain_config:
        raise ValueError(f"Unknown domain: {domain}")
    return _domain_config[domain]
```

---

## 5. ë°ì´í„° ê²€ì¦

### 5.1 ë²¡í„° ì°¨ì› ê²€ì¦

#### ì²« ë²ˆì§¸ Chunkë¡œ ê²€ì¦

```python
def validate_first_chunk(chunk: List[DomainData], domain: str):
    """ì²« Chunkë¡œ ë²¡í„° ì°¨ì› ê²€ì¦"""
    config = get_domain_config(domain)
    expected_dim = config.expected_vector_dim

    # ì²« ë²ˆì§¸ Row ê²€ì¦
    first_row = chunk[0]
    actual_dim = len(first_row.vector)

    if actual_dim != expected_dim:
        raise ValueError(
            f"Vector dimension mismatch for domain '{domain}': "
            f"expected {expected_dim}d, got {actual_dim}d"
        )

    logger.info(f"âœ… Vector dimension validated: {actual_dim}d for domain '{domain}'")
```

#### Pydantic Field Validator

```python
@dataclass
class RecruitData:
    vector: List[float]

    @field_validator('vector')
    def validate_vector_dimension(cls, v):
        """Pydantic ìë™ ê²€ì¦"""
        if len(v) != 384:
            raise ValueError(f"Expected 384d, got {len(v)}d")
        return v
```

### 5.2 í•„ìˆ˜ í•„ë“œ ê²€ì¦

```python
@dataclass
class CandidateData:
    skills: List[str]

    @field_validator('skills')
    def validate_skills(cls, v):
        """í•„ìˆ˜ í•„ë“œ ê²€ì¦"""
        if not v or len(v) == 0:
            raise ValueError("skills must not be empty")
        return v
```

### 5.3 ê°’ ë²”ìœ„ ê²€ì¦

```python
@dataclass
class RecruitData:
    exp_years: int

    @field_validator('exp_years')
    def validate_exp_years(cls, v):
        """ê°’ ë²”ìœ„ ê²€ì¦"""
        if v < 0 or v > 100:
            raise ValueError(f"exp_years must be 0-100, got {v}")
        return v
```

---

## 6. ë©”ëª¨ë¦¬ ìµœì í™”

### 6.1 Pandas ë°ì´í„° íƒ€ì… ìµœì í™”

```python
# âŒ Bad: ê¸°ë³¸ íƒ€ì… (ë©”ëª¨ë¦¬ ë‚­ë¹„)
df = pd.read_csv('data.csv')
# - 'id': object (50 bytes)
# - 'exp_years': int64 (8 bytes)
# - 'company_name': object (50 bytes)

# âœ… Good: ìµœì í™”ëœ íƒ€ì…
df = pd.read_csv(
    'data.csv',
    dtype={
        'id': str,                        # ìœ ì§€
        'exp_years': np.int16,            # 8 bytes â†’ 2 bytes (75% ì ˆê°)
        'company_name': 'category',       # 50 bytes â†’ 5 bytes (90% ì ˆê°)
        'english_level': 'category',      # 50 bytes â†’ 3 bytes (94% ì ˆê°)
        'primary_keyword': 'category'     # 50 bytes â†’ 3 bytes (94% ì ˆê°)
    }
)
```

#### ì ˆê° íš¨ê³¼

| ì»¬ëŸ¼ | ìµœì í™” ì „ | ìµœì í™” í›„ | ì ˆê°ë¥  |
|-----|----------|----------|-------|
| exp_years | int64 (8 bytes) | int16 (2 bytes) | 75% |
| company_name | object (~50 bytes) | category (~5 bytes) | 90% |
| english_level | object (~15 bytes) | category (~3 bytes) | 80% |
| primary_keyword | object (~10 bytes) | category (~3 bytes) | 70% |

**ì „ì²´ ë©”ëª¨ë¦¬ ì ˆê°**: ì•½ 40-50%

### 6.2 Vector íƒ€ì… ìµœì í™”

```python
# âŒ Bad: Python List (ì˜¤ë²„í—¤ë“œ)
vector = [0.123, 0.456, ...]  # 384 floats = ~12 KB

# âœ… Good: NumPy Array (ë©”ëª¨ë¦¬ íš¨ìœ¨)
vector = np.array([0.123, 0.456, ...], dtype=np.float32)  # 384 floats = ~1.5 KB
```

### 6.3 Chunk Size ìµœì í™”

#### ë™ì  ê³„ì‚°

```python
def calculate_optimal_chunk_size(
    vector_dimension: int,
    available_memory_mb: float = 100,
    safety_factor: float = 0.5
) -> int:
    """ë©”ëª¨ë¦¬ ê¸°ë°˜ ìµœì  Chunk í¬ê¸° ê³„ì‚°"""

    # 1. Vector ë©”ëª¨ë¦¬ (float32 = 4 bytes)
    vector_bytes = vector_dimension * 4

    # 2. Metadata ëŒ€ëµ 500 bytes
    metadata_bytes = 500

    # 3. ì´ row ë‹¹ ë©”ëª¨ë¦¬
    bytes_per_row = vector_bytes + metadata_bytes

    # 4. ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬ (ì•ˆì „ ì—¬ìœ ìœ¨)
    usable_bytes = available_memory_mb * 1024 * 1024 * safety_factor

    # 5. ìµœì  Chunk í¬ê¸°
    optimal_size = int(usable_bytes / bytes_per_row)

    # 6. ë²”ìœ„ ì œí•œ (100 ~ 10000)
    return max(100, min(optimal_size, 10000))

# ì‚¬ìš© ì˜ˆì‹œ
chunk_size = calculate_optimal_chunk_size(
    vector_dimension=384,
    available_memory_mb=100,
    safety_factor=0.5
)
# â†’ ì•½ 3000 rows
```

#### ê¶Œì¥ Chunk Size

| Vector ì°¨ì› | ë©”ëª¨ë¦¬ ì œí•œ | ê¶Œì¥ Chunk Size | ë©”ëª¨ë¦¬ ì‚¬ìš© |
|-----------|----------|----------------|-----------|
| 384 | 50 MB | 1500 | ~25 MB |
| 384 | 100 MB | 3000 | ~50 MB |
| 768 | 50 MB | 750 | ~25 MB |
| 768 | 100 MB | 1500 | ~50 MB |

---

## 7. ì‚¬ìš© ì˜ˆì‹œ

### 7.1 ê¸°ë³¸ ì‚¬ìš©

```python
from infrastructure.loaders import get_loader_auto

# 1. Loader íšë“ (ìë™ ê°ì§€)
loader = get_loader_auto("recruit", "data/recruit_embeddings.pkl")

# 2. Chunk Iterator ìƒì„±
chunks = loader.load_chunks(file_path, chunk_size=1000)

# 3. Chunk ì²˜ë¦¬
for chunk in chunks:
    print(f"Chunk size: {len(chunk)}")
    # process_chunk(chunk)
```

### 7.2 Ingestion Service í†µí•©

```python
def ingest_data_from_file(domain: str, file_name: str, chunk_size: int):
    """ë°ì´í„° ìˆ˜ì§‘ ë©”ì¸ ë¡œì§"""

    # 1. Chunk Loader íšë“
    file_path = os.path.join(DATA_DIR, file_name)
    loader = get_loader_auto(domain, file_path)

    # 2. Chunk Iterator ìƒì„±
    chunk_iter = loader.load_chunks(file_path, chunk_size)

    # 3. ì²« Chunkë¡œ ë²¡í„° ì°¨ì› ê²€ì¦
    first_chunk = next(chunk_iter)
    validate_first_chunk(first_chunk, domain)

    # 4. gRPC Client Streaming
    from infrastructure.grpc_clients import stream_data_to_batch_server

    result = stream_data_to_batch_server(
        domain=domain,
        chunks=chain([first_chunk], chunk_iter),  # ì²« chunk í¬í•¨
        metadata=metadata
    )

    return result
```

### 7.3 ìƒˆ ë„ë©”ì¸ ì¶”ê°€

```python
# Step 1: ë„ë©”ì¸ ëª¨ë¸ ì •ì˜
@dataclass
class NewDomainData:
    id: str
    field1: str
    vector: List[float]  # 512d

    @field_validator('vector')
    def validate_vector_dimension(cls, v):
        assert len(v) == 512
        return v

# Step 2: Registryì— ë“±ë¡
_loader_class_registry.update({
    ("new_domain", DataFormat.PKL): PklChunkLoader,
    ("new_domain", DataFormat.CSV): CsvChunkLoader,
})

# Step 3: ë„ë©”ì¸ ì„¤ì • ì¶”ê°€
_domain_config["new_domain"] = DomainConfig(
    expected_vector_dim=512,
    model_class=NewDomainData
)

# Step 4: ì‚¬ìš©
loader = get_loader("new_domain", DataFormat.PKL)
```

---

## 8. ì°¸ì¡°

### ê´€ë ¨ ë¬¸ì„œ
- **Python ì„œë²„ ê°œë°œ ê°€ì´ë“œ**: `/docs/Python_ì„œë²„_ê°œë°œ_ê°€ì´ë“œ.md` - ì „ì²´ ì•„í‚¤í…ì²˜
- **gRPC í†µì‹  ê°€ì´ë“œ**: `/docs/gRPC_í†µì‹ _ê°€ì´ë“œ.md` - Client Streaming

### Backend ê³µí†µ ë¬¸ì„œ (DB ìŠ¤í‚¤ë§ˆ)
- **DB ìŠ¤í‚¤ë§ˆ ê°€ì´ë“œ**: `/Backend/docs/DB_ìŠ¤í‚¤ë§ˆ_ê°€ì´ë“œ.md` â­
- **í…Œì´ë¸” ëª…ì„¸ì„œ**: `/Backend/docs/table_specification.md` â­
- **ERD ë‹¤ì´ì–´ê·¸ë¨**: `/Backend/docs/ERD_ë‹¤ì´ì–´ê·¸ë¨.md`

---

**ìµœì¢… ìˆ˜ì •ì¼**: 2025-12-17
**êµ¬í˜„ ìƒíƒœ**: Chunk Loader 3ê°€ì§€ (Pkl, Csv, Parquet) ì™„ë£Œ / 2-tier registry ì™„ë£Œ
